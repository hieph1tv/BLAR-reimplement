{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_2step_fromAllSentence.ipynb","provenance":[],"collapsed_sections":["UZfgcsaOUugf","WIF8weFhoPBc","iYhcpncKeZGr","U3zDogmTe5sB","MaTJHcDQAspA","VC8rBQDyPxf2","63yIWzUWevV0","f_kkAhRDvuv_"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Thông tin model:\n","Mô hình cho bước 2: scibert_based_cased\n","\n","- Bước 1: phân loại câu dưới dạng phân loại token gán nhãn SOBI:\n","\n","max_len = 512\n","\n","\n","- Bước 2: Phân loại token gán nhãn (BILOU, U-PR):\n","\n","max_len = 64\n"],"metadata":{"id":"zlbJdo4NNifh"}},{"cell_type":"markdown","source":["# Chuẩn bị các thư viện cần thiết"],"metadata":{"id":"UZfgcsaOUugf"}},{"cell_type":"markdown","source":["Cài đặt thư viện bioc và transformers"],"metadata":{"id":"mQE8ZQWs85S8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNgnmHf-MDG8","executionInfo":{"status":"ok","timestamp":1650945169815,"user_tz":-420,"elapsed":10844,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"def80cf0-6724-4f83-c5c5-49ff1f073cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bioc\n","  Downloading bioc-2.0.post4-py3-none-any.whl (37 kB)\n","Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 8.3 MB/s \n","\u001b[?25hCollecting lxml>=4.6.3\n","  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n","\u001b[K     |████████████████████████████████| 6.4 MB 51.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bioc) (4.64.0)\n","Collecting jsonlines>=1.2.0\n","  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: intervaltree in /usr/local/lib/python3.7/dist-packages (from bioc) (2.1.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines>=1.2.0->bioc) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines>=1.2.0->bioc) (4.2.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 87.0 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 43.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree->bioc) (2.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, lxml, jsonlines, huggingface-hub, transformers, bioc\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.2.6\n","    Uninstalling lxml-4.2.6:\n","      Successfully uninstalled lxml-4.2.6\n","Successfully installed bioc-2.0.post4 huggingface-hub-0.5.1 jsonlines-3.0.0 lxml-4.8.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"]}],"source":["pip install bioc transformers"]},{"cell_type":"code","source":["# import các thư viện\n","import os\n","import transformers\n","import bioc\n","import math\n","import torch\n","import pandas as pd\n","import numpy as np\n","from copy import deepcopy\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertForTokenClassification, AutoModelForTokenClassification\n","from sklearn.metrics import accuracy_score\n","from matplotlib import pyplot as plt"],"metadata":{"id":"ZCKoC-xHoPBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Kiểm tra colab sử dụng GPU để tăng tốc độ train\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650945173624,"user_tz":-420,"elapsed":29,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"9677f5de-9da7-4eb6-e3c9-d72e364d16cc","id":"fVtQ0PlaoPBb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["# Import data"],"metadata":{"id":"WIF8weFhoPBc"}},{"cell_type":"code","source":["#@title Thực hiện import data bằng 1 trong 2 cách\n","#@markdown Cách 1: tải folder BLAR về google drive\n","\n","#@markdown Cách 2: huấn luyện thử mô hình, tải dữ liệu về ổ cứng tạm thời của colab\n","\n","method = \"download_to_temporary_disk\" #@param [\"download_to_google_drive\", \"download_to_temporary_disk\"]\n","\n","\n","if method == 'download_to_google_drive':\n","  \n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  os.chdir('/content/drive/MyDrive/Colab Notebooks/BLAR data') ##Đổi về directory trong drive\n","\n","else:\n","  os.chdir('/content')\n","  directory = './corpus'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 16hOCtaVyuE2LK_n7mHNO_ptm_A-A8kjV\n","  !gdown --id 1Mj80pavNCcIHB7_e59DVQyv0EoS8bI6a\n","  os.chdir('/content')\n","\n","  directory = './model/model_SOBI'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 1Xg7XBdIWs5k76KEB9J1pG8HikPsIdnKM\n","  !gdown --id 1rjDB8_GmTAvBrzu8qp7TfxORl0zIMm1b\n","  !gdown --id 1MvJ0Auu0l_XrjcM9prj0xj-E9ckMXO98\n","  os.chdir('/content')\n","\n","  directory = './model/2stepFromAllSentenceModel'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 1Lut7rci259_d3waHkdRjS3MGLwBN02Pe\n","  !gdown --id 1AIBtnfEYgJlmXFBzT0kZrEVnnBfyTjTL\n","  !gdown --id 10-U7cjGOiZ6Nty7OC3BCtyswP6ilk4VY\n","  os.chdir('/content')\n","\n","## Load data từ google drive\n","### BIOADI corpus\n","from bioc import biocxml\n","with open('./corpus/bioadi_bioc_gold.xml', 'r') as fp:\n","  gold_raw = biocxml.load(fp)\n","\n","### AB3P corpus\n","from bioc import biocxml\n","with open('./corpus/Ab3P_bioc_gold.xml', 'r') as fp:\n","  ab3p = biocxml.load(fp)\n"],"metadata":{"cellView":"form","id":"XYZEx23CVudP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650960510650,"user_tz":-420,"elapsed":24574,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"beeb6434-f3ec-4b13-efc9-782493075671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=16hOCtaVyuE2LK_n7mHNO_ptm_A-A8kjV\n","To: /content/corpus/bioadi_bioc_gold.xml\n","100% 2.58M/2.58M [00:00<00:00, 232MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Mj80pavNCcIHB7_e59DVQyv0EoS8bI6a\n","To: /content/corpus/Ab3P_bioc_gold.xml\n","100% 2.36M/2.36M [00:00<00:00, 246MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Xg7XBdIWs5k76KEB9J1pG8HikPsIdnKM\n","To: /content/model/model_SOBI/config.json\n","100% 883/883 [00:00<00:00, 1.42MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1rjDB8_GmTAvBrzu8qp7TfxORl0zIMm1b\n","To: /content/model/model_SOBI/pytorch_model.bin\n","100% 437M/437M [00:01<00:00, 246MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1MvJ0Auu0l_XrjcM9prj0xj-E9ckMXO98\n","To: /content/model/model_SOBI/vocab.txt\n","100% 222k/222k [00:00<00:00, 105MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Lut7rci259_d3waHkdRjS3MGLwBN02Pe\n","To: /content/model/2stepFromAllSentenceModel/config.json\n","100% 959/959 [00:00<00:00, 1.91MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1AIBtnfEYgJlmXFBzT0kZrEVnnBfyTjTL\n","To: /content/model/2stepFromAllSentenceModel/pytorch_model.bin\n","100% 437M/437M [00:01<00:00, 295MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=10-U7cjGOiZ6Nty7OC3BCtyswP6ilk4VY\n","To: /content/model/2stepFromAllSentenceModel/vocab.txt\n","100% 222k/222k [00:00<00:00, 70.8MB/s]\n"]}]},{"cell_type":"code","source":["# Tạo corpus phụ gồm các văn bản không chứa từ viết tắt\n","n = len(gold_raw.documents)\n","noAcronym = []\n","gold_noAcronym = bioc.bioc.BioCCollection()\n","for i, document in enumerate(gold_raw.documents):\n","  if len(document.passages[0].annotations) == 0:\n","    gold_noAcronym.add_document(document)\n","    noAcronym.append(i)"],"metadata":{"id":"9O77CO-QCsx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tạo corpus phụ gồm các văn bản chứa từ viết tắt\n","n = len(gold_raw.documents)\n","Acronym = []\n","gold = bioc.bioc.BioCCollection()\n","for i, document in enumerate(gold_raw.documents):\n","  if len(document.passages[0].annotations) != 0:\n","    gold.add_document(document)\n","    Acronym.append(i)"],"metadata":{"id":"NSwvknWig941"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load SciBERT model"],"metadata":{"id":"iYhcpncKeZGr"}},{"cell_type":"markdown","source":["Load pretrained SciBERT Model"],"metadata":{"id":"E3eyJzGjrRFj"}},{"cell_type":"code","source":["tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n","scibert_model = transformers.AutoModel.from_pretrained('allenai/scibert_scivocab_cased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a70sTDySeYzr","executionInfo":{"status":"ok","timestamp":1650945879445,"user_tz":-420,"elapsed":3409,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"162b9302-92fa-4ca4-a7e6-d7c5f828f080"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# sample tokenization\n","sample = gold.documents[0].passages[0].text\n","encoding = tokenizer.encode(sample,\n","                            return_offsets_mapping=True, \n","                            padding='max_length', \n","                            truncation=True, \n","                            max_length=512)\n","\n","print(encoding)\n","print(tokenizer.convert_ids_to_tokens(encoding))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QudmTBS7jYR2","executionInfo":{"status":"ok","timestamp":1650945879446,"user_tz":-420,"elapsed":33,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"c65fb6d9-6265-4021-bb4d-7a73bbf1a108"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 105, 1174, 1291, 5839, 11986, 8589, 307, 864, 24820, 3254, 16737, 430, 2107, 1422, 136, 9297, 1004, 648, 211, 5839, 11986, 8589, 640, 231, 2771, 5922, 202, 1682, 201, 3683, 6990, 146, 8676, 111, 22330, 6746, 5839, 11986, 26164, 30111, 1820, 1185, 1066, 125, 5839, 11986, 26164, 30111, 231, 1109, 188, 105, 4035, 125, 3918, 211, 18221, 289, 2306, 111, 24820, 3254, 16737, 125, 105, 1291, 2475, 1538, 324, 7900, 5839, 11986, 26164, 578, 1174, 5839, 11986, 8589, 307, 143, 160, 22051, 10936, 30108, 551, 211, 111, 4097, 143, 262, 711, 134, 19304, 30108, 136, 825, 1066, 551, 163, 4434, 3055, 124, 4006, 430, 596, 188, 4712, 659, 21449, 211, 111, 160, 22051, 10936, 30108, 8881, 10369, 5839, 11986, 17511, 16539, 124, 711, 28880, 136, 1755, 23936, 30112, 3581, 430, 136, 9904, 4777, 15344, 10670, 136, 1487, 10409, 9615, 430, 596, 319, 21002, 2337, 211, 4237, 9297, 731, 3407, 202, 111, 160, 22051, 10936, 30108, 11050, 803, 146, 111, 1755, 23936, 30112, 1487, 136, 797, 146, 111, 4284, 1642, 201, 111, 1487, 211, 111, 9297, 1513, 1109, 188, 306, 1487, 10409, 9297, 476, 231, 1198, 430, 136, 305, 163, 7395, 202, 160, 22051, 10936, 30108, 441, 105, 1267, 4284, 2350, 2358, 136, 105, 1487, 3944, 2358, 211, 111, 4777, 15344, 10670, 1065, 163, 2002, 1158, 146, 111, 1511, 125, 4284, 6172, 211, 124, 2187, 146, 1280, 750, 578, 1174, 2708, 9376, 5922, 202, 5907, 5839, 11986, 8589, 307, 1088, 430, 160, 22051, 10936, 30108, 8881, 10369, 645, 5839, 11986, 26164, 30111, 136, 441, 537, 521, 700, 12342, 1088, 211, 125, 2914, 1650, 430, 160, 22051, 10936, 30108, 1484, 319, 1682, 201, 10097, 115, 663, 16539, 211, 125, 111, 1515, 5621, 2769, 430, 645, 4851, 508, 28958, 21757, 8225, 11986, 16521, 143, 1170, 21324, 551, 9668, 171, 136, 16794, 16018, 10144, 111, 12342, 1088, 211, 111, 3641, 224, 1170, 21324, 163, 2670, 188, 111, 9714, 2327, 1356, 173, 111, 4097, 578, 105, 16063, 2090, 1856, 30105, 188, 105, 9714, 1225, 223, 5769, 125, 958, 578, 16463, 430, 193, 30119, 578, 23174, 136, 2311, 578, 23199, 211, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['[CLS]', 'a', 'specific', 'human', 'lys', '##ophosph', '##olip', '##ase', ':', 'cd', '##na', 'cloning', ',', 'tissue', 'distribution', 'and', 'kinetic', 'character', '##ization', '.', 'lys', '##ophosph', '##olip', '##ases', 'are', 'critical', 'enzymes', 'that', 'act', 'on', 'biological', 'membranes', 'to', 'regulate', 'the', 'multif', '##unctional', 'lys', '##ophosph', '##olipid', '##s', ';', 'increased', 'levels', 'of', 'lys', '##ophosph', '##olipid', '##s', 'are', 'associated', 'with', 'a', 'host', 'of', 'diseases', '.', 'herein', 'we', 'report', 'the', 'cd', '##na', 'cloning', 'of', 'a', 'human', 'brain', '25', 'k', '##da', 'lys', '##ophosph', '##olipid', '-', 'specific', 'lys', '##ophosph', '##olip', '##ase', '(', 'h', '##lys', '##opl', '##a', ')', '.', 'the', 'enzyme', '(', 'at', 'both', 'm', '##rn', '##a', 'and', 'protein', 'levels', ')', 'is', 'widely', 'distributed', 'in', 'tissues', ',', 'but', 'with', 'quite', 'different', 'abundances', '.', 'the', 'h', '##lys', '##opl', '##a', 'hydroly', '##zes', 'lys', '##ophosph', '##atidyl', '##choline', 'in', 'both', 'monomeric', 'and', 'mice', '##lla', '##r', 'forms', ',', 'and', 'exhibits', 'apparent', 'cooper', '##ativity', 'and', 'surface', 'dilution', 'kinetics', ',', 'but', 'not', 'interfacial', 'activation', '.', 'detailed', 'kinetic', 'analysis', 'indicates', 'that', 'the', 'h', '##lys', '##opl', '##a', 'binds', 'first', 'to', 'the', 'mice', '##lla', '##r', 'surface', 'and', 'then', 'to', 'the', 'substrate', 'presented', 'on', 'the', 'surface', '.', 'the', 'kinetic', 'parameters', 'associated', 'with', 'this', 'surface', 'dilution', 'kinetic', 'model', 'are', 'reported', ',', 'and', 'it', 'is', 'concluded', 'that', 'h', '##lys', '##opl', '##a', 'has', 'a', 'single', 'substrate', 'binding', 'site', 'and', 'a', 'surface', 'recognition', 'site', '.', 'the', 'apparent', 'cooper', '##ativity', 'observed', 'is', 'likely', 'due', 'to', 'the', 'change', 'of', 'substrate', 'presentation', '.', 'in', 'contrast', 'to', 'many', 'non', '-', 'specific', 'lip', '##olytic', 'enzymes', 'that', 'exhibit', 'lys', '##ophosph', '##olip', '##ase', 'activity', ',', 'h', '##lys', '##opl', '##a', 'hydroly', '##zes', 'only', 'lys', '##ophosph', '##olipid', '##s', 'and', 'has', 'no', 'other', 'significant', 'enzymatic', 'activity', '.', 'of', 'special', 'interest', ',', 'h', '##lys', '##opl', '##a', 'does', 'not', 'act', 'on', 'plasm', '##en', '##yl', '##choline', '.', 'of', 'the', 'several', 'inhibitors', 'tested', ',', 'only', 'methyl', 'ar', '##achid', '##onyl', 'fluor', '##ophosph', '##onate', '(', 'ma', '##fp', ')', 'potent', '##ly', 'and', 'irrever', '##sibly', 'inhibits', 'the', 'enzymatic', 'activity', '.', 'the', 'inhibition', 'by', 'ma', '##fp', 'is', 'consistent', 'with', 'the', 'catalytic', 'mechanism', 'proposed', 'for', 'the', 'enzyme', '-', 'a', 'serine', 'hydro', '##las', '##e', 'with', 'a', 'catalytic', 'tri', '##ad', 'composed', 'of', 'ser', '-', '119', ',', 'as', '##p', '-', '174', 'and', 'his', '-', '208', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}]},{"cell_type":"markdown","source":["# Load sentence classification model"],"metadata":{"id":"EnqeGxt43otb"}},{"cell_type":"markdown","source":["### load model"],"metadata":{"id":"U3zDogmTe5sB"}},{"cell_type":"code","source":["# Parameter cho SOBI model\n","MAX_LEN = 512\n","labels_to_ids_sentence = {'B':0,'I':1,'S':2, 'O':3,'SpecialToken':-100}\n","ids_to_labels_sentence = {0:'B',1:'I',2:'S', 3: 'O', -100:'SpecialToken'}"],"metadata":{"id":"MTQ5ERFxhBAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = \"./model/model_SOBI\"\n","tokenizer = transformers.AutoTokenizer.from_pretrained(directory, local_files_only=True)\n","model_seq = transformers.AutoModelForTokenClassification.from_pretrained(directory, local_files_only=True)\n","model_seq.to(device)"],"metadata":{"id":"W7K3RgKLe5E4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650945896992,"user_tz":-420,"elapsed":12162,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"15e45e54-d882-4e1e-b9df-0fc82ecd7626"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["#@title Functions\n","#@markdown Load các hàm cho mô hình phân loại câu\n","\n","\n","def ExtractSentenceWithAcronym(original_passage):\n","  # this function will return new passage with annotated sentences\n","\n","\n","  passage = deepcopy(original_passage)\n","  text = passage.text\n","  # extract annotation\n","  sf_offset_stack_unsort = dict()\n","  for i, annotation in enumerate(passage.annotations):\n","    if i%2 == 0:\n","      sf_offset_stack_unsort[passage.annotations[i].total_span.offset] = i #extract the SF offset only\n","  offsetList = [offset for offset,i in sf_offset_stack_unsort.items()]\n","  \n","  sf_offset_stack = []\n","  while offsetList:\n","    minoffset = min(offsetList)\n","    idx = sf_offset_stack_unsort[minoffset]\n","    sf_offset_stack.append([idx, passage.annotations[idx].total_span.offset])\n","    offsetList.remove(minoffset)\n","\n","  \n","  # Sentence extraction\n","  ## Presumption: \n","  ### SF always stands before LF in corpus annotation list\n","  ### Sentence extraction by: '. '\n","  ### Sentence starts with CASED character\n","  ### Sentence length > 20 characters\n","\n","  rawSentenceList = text.split('. ') #split passage to list of sentence\n","  cased_alphabet = 'ABCEDEFGHIJKLMNOPQRSTUVWXYZ'\n","  sentenceList = []\n","  for i, raw_sentence in enumerate(rawSentenceList):\n","    if raw_sentence[-1] != '.':\n","      raw_sentence += '.' # add . if sentence doesn't have\n","    if raw_sentence[0] in cased_alphabet and len(raw_sentence) >10:\n","      sentenceList.append(raw_sentence)\n","    else:\n","      if sentenceList:\n","        sentenceList[-1] += ' ' + raw_sentence\n","      else:\n","        sentenceList.append(raw_sentence)\n","\n","\n","  # Add annotation for sentence\n","  pointer = 0\n","  for sentence in sentenceList:\n","    bioc_sentence = bioc.bioc.BioCSentence()\n","    bioc_sentence.text = sentence\n","    while len(sf_offset_stack):\n","      if sf_offset_stack[0][1] < pointer + len(sentence):\n","        sf = sf_offset_stack.pop(0)\n","        bioc_sentence.annotations.append(passage.annotations[sf[0]]) # get SF annotation\n","        bioc_sentence.annotations.append(passage.annotations[sf[0]+1]) # get LF annotation\n","        bioc_sentence.annotations[-2].locations[0].offset -= pointer # change SF offset to sentence (originally was offset passage)\n","        bioc_sentence.annotations[-1].locations[0].offset -= pointer # change LF offset to sentence (originally was offset passage)  \n","      else:\n","        break\n","    bioc_sentence.offset = pointer\n","    pointer += len(sentence) + 1 # move pointer to next sentence, 1 for .\n","    passage.add_sentence(bioc_sentence) # add sentence to passage\n","      \n","  return passage\n","        \n","\n","def PredictTokenSentence(text):\n","  # This function returns predictions of token in Sentence Classification task\n","  text = text.split(' ')\n","  inputs = tokenizer(text,\n","                    is_split_into_words=True,\n","                    return_offsets_mapping=True, \n","                    padding='max_length', \n","                    truncation=True, \n","                    max_length=512,\n","                    return_tensors=\"pt\")\n","  # move to gpu\n","  ids = inputs[\"input_ids\"].to(device)\n","  mask = inputs[\"attention_mask\"].to(device)\n","  # forward pass\n","  outputs = model_seq(ids, attention_mask=mask)\n","  logits = outputs[0]\n","\n","  active_logits = logits.view(-1, model_seq.num_labels) # shape (batch_size * seq_len, num_labels)\n","  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","  tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","  token_predictions = [ids_to_labels_sentence[i] for i in flattened_predictions.cpu().numpy()]\n","  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","  prediction = []\n","  for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n","    #only predictions on first word pieces are important\n","    if mapping[0] == 0 and mapping[1] != 0:\n","      prediction.append(token_pred[1])\n","    else:\n","      continue\n","  return prediction\n","\n","def PredictSentenceWithABBR(text):\n","  # This function returns list of predicted correct sentence with LF/SF pairs\n","  text_split = text.split(' ')\n","  prediction = PredictTokenSentence(text)\n","  sentenceList = []\n","  for i in range(len(prediction)-1):\n","    if prediction[i] == 'B':\n","      for j in range(i+1, len(prediction)):\n","        if prediction[j] in 'BS':\n","          sentence = ' '.join(text_split[i:j])\n","          if j-i>4: # at least 5 words per sentence\n","            sentenceList.append(sentence)\n","          break\n","        elif j == len(prediction)-1:\n","          sentence = ' '.join(text_split[i:])\n","          sentenceList.append(sentence)\n","          break               \n","  return sentenceList\n","  \n","def PredictBiocSentenceWithABBR(passage):\n","  # This function return predicted BioC formated sentences form Gold corpus\n","  sentences_from_passage = ExtractSentenceWithAcronym(passage).sentences\n","  sentenceList = PredictSentenceWithABBR(passage.text)\n","  biocSentenceList = []\n","  for sentence in sentences_from_passage:\n","    for i, annotation in enumerate(sentence.annotations):\n","      if i%2 == 0 :\n","        haveSF = sentence.annotations[0].text in ' '.join(PredictSentenceWithABBR(passage.text))\n","        haveLF = []\n","        for word in sentence.annotations[1].text.split(' '):\n","          if word in ' '.join(PredictSentenceWithABBR(passage.text)):\n","            haveLF.append(1)\n","          else:\n","            haveLF.append(0)\n","        haveLF = bool(np.prod(haveLF))\n","        if haveSF and haveLF:\n","          if sentence not in biocSentenceList:\n","            biocSentenceList.append(sentence)\n","\n","    if not sentence.annotations:\n","      if sentence.text in sentenceList:\n","        biocSentenceList.append(sentence)\n","      else:\n","        for predicted_sentence in sentenceList:\n","          if predicted_sentence in sentence.text:\n","            biocSentenceList.append(sentence)\n","\n","  return biocSentenceList"],"metadata":{"cellView":"form","id":"psG7pBobsCzN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create corpus for step 2 token classification model with tagged token "],"metadata":{"id":"MaTJHcDQAspA"}},{"cell_type":"code","source":["#@title Functions\n","def BIOTagSequenceWithAcronym(original_passage):\n","  # this function will return list of token's labels\n","  passage = deepcopy(original_passage)\n","  text = passage.text\n","  # extract annotation\n","  sf_offset_stack = []\n","  for i, annotation in enumerate(passage.annotations):\n","    if i%2 == 0:\n","      sf_offset_stack.append([i,passage.annotations[i].total_span.offset]) #extract the SF offset only\n","\n","  # Sentence extraction\n","  ## Presumption: \n","  ### SF always stands before LF in corpus annotation list\n","  ### Sentence extraction by: '. '\n","  ### Sentence starts with CASED character\n","  ### Sentence length > 20 characters\n","\n","  rawSentenceList = text.split('. ') #split passage to list of sentence\n","  \n","  cased_alphabet = 'ABCEDEFGHIJKLMNOPQRSTUVWXYZ'\n","  sentenceList = []\n","  for i, raw_sentence in enumerate(rawSentenceList):\n","    if raw_sentence[-1] != '.':\n","      raw_sentence += '.' # add . if sentence doesn't have\n","    if raw_sentence[0] in cased_alphabet and len(raw_sentence) >20:\n","      sentenceList.append(raw_sentence)\n","    else:\n","      if sentenceList:\n","        sentenceList[-1] += ' ' + raw_sentence\n","      else:\n","        sentenceList.append(raw_sentence)\n","  \n","  # add annotation for sentence\n","  pointer = 0\n","  sentenceTag = ['NOTcontainSFLF']*len(sentenceList)\n","  for i,sentence in enumerate(sentenceList):\n","    while len(sf_offset_stack):\n","      if sf_offset_stack[0][1] < pointer + len(sentence):\n","        sf = sf_offset_stack.pop(0)\n","        sentenceTag[i]= 'containSFLF'  \n","      else:    \n","        break\n","    \n","    pointer += len(sentence) + 1 # move pointer to next sentence, 1 for .\n","\n","\n","\n","  wordTag = []\n","  for i,sentence in enumerate(sentenceList):\n","    sentence_wordlength = len(sentence.split(' '))\n","    if sentenceTag[i] == 'containSFLF':\n","      wordTag += 'B'\n","      wordTag += ['I']*(sentence_wordlength-1)\n","    elif sentenceTag[i] == 'NOTcontainSFLF':\n","      wordTag += 'S'\n","      wordTag += ['O']*(sentence_wordlength-1)\n","  return wordTag\n","        "],"metadata":{"id":"k6KFqgcIBffP","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title create all sentence corpus\n","#@markdown Tạo training corpus dưới dạng tất cả các câu chứa từ viết tắt/định nghĩa hoặc không\n","sentence_corpus = []\n","tag_corpus = []\n","\n","for i,document in enumerate(gold_raw.documents):\n","  sentence_corpus.append(document.passages[0].text)\n","  tag_corpus.append(BIOTagSequenceWithAcronym(document.passages[0]))\n","sentence_df = pd.DataFrame({'sentence':sentence_corpus,'labels':tag_corpus})\n","\n","\n","Token_datalist = [] \n","for i in range(sentence_df.shape[0]):\n","  Token_datalist += ExtractSentenceWithAcronym(gold_raw.documents[i].passages[0]).sentences"],"metadata":{"id":"myJRpK2Oc9O4","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Functions for BILOU tag\n","def BILOUTagForCharacter(passage):\n","  text = passage.text\n","  biloutag = ['O']*len(text)\n","  annotations = passage.annotations\n","\n","  for i, annotation in enumerate(annotations):\n","    if i % 2 == 0:\n","\n","      biloutag[annotation.total_span.offset : annotation.total_span.offset + annotation.total_span.length] = ['U-SF']*annotation.total_span.length\n","      if text[annotation.total_span.offset-1] == '(':\n","        biloutag[annotation.total_span.offset-1] = 'U-PR'\n","        biloutag[annotation.total_span.offset + annotation.total_span.length] = 'U-PR'\n","    else:\n","      biloutag[annotation.locations[0].offset] = 'B'\n","      biloutag[annotation.locations[0].offset + annotation.locations[0].length-1] = 'L'\n","      biloutag[annotation.locations[0].offset + 1 : annotation.locations[0].offset + annotation.total_span.length-1] = ['I']*(annotation.total_span.length-2)\n","  for i,c in enumerate(text):\n","    if c == ' ':\n","      biloutag[i] = 'SPACE'\n","  return biloutag\n","\n","def TokenBILOU(token_ids, characterBILOU,tokenizer= tokenizer):\n","\n","  noSpaceCharacterBILOU = [character for character in characterBILOU if character != 'SPACE']\n","  \n","  tokenList = tokenizer.convert_ids_to_tokens(token_ids['input_ids'])\n","  character_count = 0\n","  tokenTag = []\n","\n","  for token in tokenList:\n","    if token[0:2] == '##':\n","      token = token[2:]\n","    elif token[0] == '[':\n","      token = ''\n","      tokenTag.append('TAG')\n","      continue\n","    n = len(token)\n","    extractTag = noSpaceCharacterBILOU[character_count:character_count+n]\n","    if extractTag[0] == 'B':\n","      tokenTag.append('B-LF')\n","    elif extractTag[0] == 'U-PR':\n","      tokenTag.append('U-PR')\n","    elif extractTag[0] == 'U-SF':\n","      tokenTag.append('U-SF')\n","    elif extractTag[0] == 'L':\n","      tokenTag.append('L-LF')\n","    elif extractTag[0] == 'O':\n","      tokenTag.append('O')\n","    elif extractTag[0] == 'I':\n","      if extractTag[-1] == 'L':\n","        tokenTag.append('L-LF')\n","      else:\n","        tokenTag.append('I-LF')\n","\n","    character_count += n\n","\n","  return tokenTag\n","\n","\n","# Tagging scheme\n","labels_to_ids = {'B-LF':0,\n","                 'I-LF':1,\n","                 'L-LF':2,\n","                 'O':3,\n","                 'U-SF':4,\n","                 'U-PR':5,\n","                 'TAG':6}\n","ids_to_labels = {v:k for k,v in labels_to_ids.items()}"],"metadata":{"id":"OQL9X3A59_QE","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot histogram of sentence length\n","plt.hist([len(sentence.text.split(' ')) for sentence in Token_datalist])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylqNkTO9KfIj","executionInfo":{"status":"ok","timestamp":1650945898155,"user_tz":-420,"elapsed":674,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"eb8a1b17-c586-4ecd-e68b-570593c65f5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([2.741e+03, 6.438e+03, 1.483e+03, 2.120e+02, 2.900e+01, 1.100e+01,\n","        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n"," array([  1. ,  15.6,  30.2,  44.8,  59.4,  74. ,  88.6, 103.2, 117.8,\n","        132.4, 147. ]),\n"," <a list of 10 Patch objects>)"]},"metadata":{},"execution_count":32},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["MAX_LEN = 64\n","TRAIN_BATCH_SIZE = 128\n","VALID_BATCH_SIZE = 4\n","EPOCHS = 5\n","LEARNING_RATE = 3e-06\n","MAX_GRAD_NORM = 10"],"metadata":{"id":"-TRV1hQWVWdI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TokenClassification model"],"metadata":{"id":"VC8rBQDyPxf2"}},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"63yIWzUWevV0"}},{"cell_type":"code","source":["# create dataframe\n","datalist = []\n","for i, sentence in enumerate(Token_datalist):\n","  datalist.append([sentence.text, BILOUTagForCharacter(sentence)])\n","dataframe = pd.DataFrame(datalist, columns = ['text','character_bilou'])"],"metadata":{"id":"K43sTayHevV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset class for input to model\n","class dataset(Dataset):\n","  def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","  def __getitem__(self, index):\n","        # step 1: get the sentence and word labels \n","        text = self.data.text[index] \n","        character_labels = self.data.character_bilou[index]\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(text, \n","                                  return_offsets_mapping=True, \n","                                  padding='max_length', \n","                                  truncation=True, \n","                                  max_length=self.max_len)\n","        \n","        labels = TokenBILOU(encoding, character_labels)\n","        encoded_labels = [labels_to_ids[label] for label in labels]\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","       \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"metadata":{"id":"k35bF8_hI43D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train test split\n","train_size = 0.8\n","train_dataset = dataframe.sample(frac=train_size,random_state=200)\n","test_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(dataframe.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n","testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"],"metadata":{"id":"-2uCn8Bv8UHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650945898159,"user_tz":-420,"elapsed":13,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"baa7d74e-f059-4d0d-e66a-ff7188682b76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FULL Dataset: (10916, 2)\n","TRAIN Dataset: (8733, 2)\n","TEST Dataset: (2183, 2)\n"]}]},{"cell_type":"code","source":["# Check if dataset is correctly labeled\n","for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[2][\"input_ids\"]), training_set[2][\"labels\"]):\n","  print('{0:10}  {1}'.format(token, label))"],"metadata":{"id":"J6UcdmjhUNQp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650945898498,"user_tz":-420,"elapsed":351,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"5c2291bf-4eac-4a9c-c087-29449c132925"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]       6\n","we          3\n","also        3\n","found       3\n","more        3\n","alternatively  3\n","splice      3\n","##d         3\n","exons       3\n","that        3\n","can         3\n","give        3\n","rise        3\n","to          3\n","heterogeneous  3\n","transcripts  3\n",".           3\n","[SEP]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n","[PAD]       6\n"]}]},{"cell_type":"markdown","source":["## Model \n","Thực hiện cell này để train lại model"],"metadata":{"id":"CGRRpAftURC_"}},{"cell_type":"markdown","source":["####Preparation"],"metadata":{"id":"f_kkAhRDvuv_"}},{"cell_type":"code","source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"metadata":{"id":"78mZwl4NUXvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load pretrained model with new output_length\n","model = AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_cased', num_labels=len(labels_to_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFtW_fv7T8tL","executionInfo":{"status":"ok","timestamp":1650945911055,"user_tz":-420,"elapsed":1981,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"1d264e8a-0c2b-49ea-afd3-98d6a41cbdd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OoluElGVEO1","executionInfo":{"status":"ok","timestamp":1650945911056,"user_tz":-420,"elapsed":34,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"b0d4bbf0-8439-4fd3-f21a-28019503235e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["# sample input before training\n","inputs = training_set[16]\n","input_ids = inputs[\"input_ids\"].unsqueeze(0)\n","attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n","labels = inputs[\"labels\"].unsqueeze(0)\n","\n","input_ids = input_ids.to(device)\n","attention_mask = attention_mask.to(device)\n","labels = labels.to(device)\n","\n","outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","initial_loss = outputs[0]\n","initial_loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObIUYDHcU-0y","executionInfo":{"status":"ok","timestamp":1650945911057,"user_tz":-420,"elapsed":24,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"b21187b7-8ce1-4b60-d607-e52310979726"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.8617, device='cuda:0', grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# check logits if model has random prediction\n","-math.log(1/8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDFhHTiljN-q","executionInfo":{"status":"ok","timestamp":1650945911058,"user_tz":-420,"elapsed":21,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"1f881d21-a9c1-4169-9993-1d29e9858cc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0794415416798357"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["####Training"],"metadata":{"id":"OQ4_-8_G_3xu"}},{"cell_type":"code","source":["#@title Optimizer\n","LEARNING_RATE = 3e-5 #@param {type:\"number\"}\n","optimizer = 'Adam' #@param {type:'string'}\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"rHwgy36VkQDR","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define traning function\n","# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(training_loader):\n","  \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        \n","        tr_logits = outputs[1]\n","        loss = outputs[0]\n","\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 2==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss at training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != 6 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")"],"metadata":{"id":"DVne4G7qkT26","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Train\n","EPOCHS = 5 #@param {type:\"integer\"}\n","for epoch in range(EPOCHS):\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lkWX233Tkh6R","executionInfo":{"status":"error","timestamp":1650946140945,"user_tz":-420,"elapsed":229902,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"1ecc65af-da8d-4bae-9f98-79c48e70c19c","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss at training steps: 1.9936645030975342\n","Training loss at training steps: 1.4928655624389648\n","Training loss at training steps: 1.1530862689018249\n","Training loss at training steps: 0.9121106692722866\n","Training loss at training steps: 0.7548153383864297\n","Training loss at training steps: 0.6472382355820049\n","Training loss at training steps: 0.5670839015107888\n","Training loss at training steps: 0.5149315024415652\n","Training loss at training steps: 0.46639549030977134\n","Training loss at training steps: 0.43386123917604746\n","Training loss at training steps: 0.4036850201941672\n","Training loss at training steps: 0.37904789460741956\n","Training loss at training steps: 0.358120718896389\n","Training loss at training steps: 0.34042291977891215\n","Training loss at training steps: 0.32173575024152623\n","Training loss at training steps: 0.3061665979123885\n","Training loss at training steps: 0.29180707884105767\n","Training loss at training steps: 0.2797163802598204\n","Training loss at training steps: 0.26851903798209653\n","Training loss at training steps: 0.2588532462907143\n","Training loss at training steps: 0.24863826192733718\n","Training loss at training steps: 0.2405003652323124\n","Training loss at training steps: 0.2324340510699484\n","Training loss at training steps: 0.22438125970198752\n","Training loss at training steps: 0.2169151586689511\n","Training loss at training steps: 0.20985878310074993\n","Training loss at training steps: 0.20339604678019038\n","Training loss at training steps: 0.19737565950913863\n","Training loss at training steps: 0.19175291159435323\n","Training loss at training steps: 0.18626042632228237\n","Training loss at training steps: 0.1815499862442251\n","Training loss at training steps: 0.1765719267229239\n","Training loss at training steps: 0.17243451163745843\n","Training loss at training steps: 0.16793965275830297\n","Training loss at training steps: 0.16420402178081914\n","Training loss epoch: 0.16420402178081914\n","Training accuracy epoch: 0.9274449663318846\n","Training epoch: 2\n","Training loss at training steps: 0.02100626565515995\n","Training loss at training steps: 0.02091796137392521\n","Training loss at training steps: 0.023455531522631645\n","Training loss at training steps: 0.02381922278021063\n","Training loss at training steps: 0.023985633419619665\n","Training loss at training steps: 0.0225435791706497\n","Training loss at training steps: 0.02382647518355113\n","Training loss at training steps: 0.02482060417532921\n","Training loss at training steps: 0.024521870757727063\n","Training loss at training steps: 0.02344073754686274\n","Training loss at training steps: 0.02299523014309151\n","Training loss at training steps: 0.022449684754499922\n","Training loss at training steps: 0.022541618179529904\n","Training loss at training steps: 0.02249091803268702\n","Training loss at training steps: 0.022446972944227785\n","Training loss at training steps: 0.022632721165615703\n","Training loss at training steps: 0.022334085622181494\n","Training loss at training steps: 0.022273101032312427\n","Training loss at training steps: 0.02206796522227091\n","Training loss at training steps: 0.022281502815297782\n","Training loss at training steps: 0.022016851356389318\n","Training loss at training steps: 0.021952543136945297\n","Training loss at training steps: 0.02189310910180211\n","Training loss at training steps: 0.021779567766142018\n","Training loss at training steps: 0.021638747266664798\n","Training loss at training steps: 0.021642142760695194\n","Training loss at training steps: 0.02131030478356582\n","Training loss at training steps: 0.021130557713860815\n","Training loss at training steps: 0.02098275098557535\n","Training loss at training steps: 0.020726718134799246\n","Training loss at training steps: 0.020566268121732064\n","Training loss at training steps: 0.020339806076316608\n","Training loss at training steps: 0.020588860445870803\n","Training loss at training steps: 0.0204319751807558\n","Training loss at training steps: 0.020196443916284952\n","Training loss epoch: 0.020196443916284952\n","Training accuracy epoch: 0.9888337779766738\n","Training epoch: 3\n","Training loss at training steps: 0.015839247032999992\n","Training loss at training steps: 0.009635074917847911\n","Training loss at training steps: 0.010743109369650483\n","Training loss at training steps: 0.012093714604686414\n","Training loss at training steps: 0.012202812229386635\n","Training loss at training steps: 0.01187892349182882\n","Training loss at training steps: 0.01173868583730207\n","Training loss at training steps: 0.011976959199334185\n","Training loss at training steps: 0.01130065301378422\n","Training loss at training steps: 0.01132769474612647\n","Training loss at training steps: 0.011610906072227018\n","Training loss at training steps: 0.01131228868768591\n","Training loss at training steps: 0.01149843462742865\n","Training loss at training steps: 0.012299606516198427\n","Training loss at training steps: 0.012466995631633648\n","Training loss at training steps: 0.013090079627750863\n","Training loss at training steps: 0.012836243456342456\n","Training loss at training steps: 0.012962999919961606\n","Training loss at training steps: 0.012655882435422894\n","Training loss at training steps: 0.012439987580411328\n","Training loss at training steps: 0.012601873275210581\n","Training loss at training steps: 0.012510618492744343\n","Training loss at training steps: 0.012569635480435357\n","Training loss at training steps: 0.012709178461475259\n","Training loss at training steps: 0.012850856175646186\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-366673a66b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-fa0369506690>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#@title Evaluation\n","def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            \n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            \n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 20==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 20 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")\n","\n","    return labels, predictions\n","labels, predictions = valid(model, testing_loader)"],"metadata":{"id":"If2Vz4NionZz","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650946214523,"user_tz":-420,"elapsed":70929,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"c6eb8bac-595a-47bd-d527-e9474f7748ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss per 20 evaluation steps: 0.023345759138464928\n","Validation loss per 20 evaluation steps: 0.07870708657927546\n","Validation loss per 20 evaluation steps: 0.04985581991822095\n","Validation loss per 20 evaluation steps: 0.03514025227136177\n","Validation loss per 20 evaluation steps: 0.027975429951272227\n","Validation loss per 20 evaluation steps: 0.026407252391022262\n","Validation loss per 20 evaluation steps: 0.02531862613232229\n","Validation loss per 20 evaluation steps: 0.023142984185135818\n","Validation loss per 20 evaluation steps: 0.021366173195850064\n","Validation loss per 20 evaluation steps: 0.024248603719977394\n","Validation loss per 20 evaluation steps: 0.02374020032056165\n","Validation loss per 20 evaluation steps: 0.024448584549287566\n","Validation loss per 20 evaluation steps: 0.023858698390902957\n","Validation loss per 20 evaluation steps: 0.02303181583843715\n","Validation loss per 20 evaluation steps: 0.022011451206080536\n","Validation loss per 20 evaluation steps: 0.02134579332488159\n","Validation loss per 20 evaluation steps: 0.025143664894278113\n","Validation loss per 20 evaluation steps: 0.024583019237377123\n","Validation loss per 20 evaluation steps: 0.023829125760702306\n","Validation loss per 20 evaluation steps: 0.022738165295593627\n","Validation loss per 20 evaluation steps: 0.022521044922555085\n","Validation loss per 20 evaluation steps: 0.02227750282382483\n","Validation loss per 20 evaluation steps: 0.02179138205177705\n","Validation loss per 20 evaluation steps: 0.021247316776611187\n","Validation loss per 20 evaluation steps: 0.0211341274381744\n","Validation loss per 20 evaluation steps: 0.02197323761498853\n","Validation loss per 20 evaluation steps: 0.021492516845979748\n","Validation loss per 20 evaluation steps: 0.021854553534482544\n","Validation Loss: 0.021710099703421214\n","Validation Accuracy: 0.994088160103785\n"]}]},{"cell_type":"markdown","source":["#### Save model"],"metadata":{"id":"PsIl0BSLsCfc"}},{"cell_type":"code","source":["directory = \"./model/newstep2model\" # Đặt tên cho model\n","\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')"],"metadata":{"id":"fNU6vEiesEMd","executionInfo":{"status":"ok","timestamp":1650946226405,"user_tz":-420,"elapsed":1263,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68093071-3010-4988-e331-6ceddd881b2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n"]}]},{"cell_type":"markdown","source":["Load model"],"metadata":{"id":"8HHOHovNAcRw"}},{"cell_type":"code","source":["directory = \"./model/newstep2model\"\n","tokenizer = transformers.AutoTokenizer.from_pretrained(directory, local_files_only=True)\n","model = transformers.AutoModelForTokenClassification.from_pretrained(directory, local_files_only=True)\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9Qesj1g0z_9","executionInfo":{"status":"ok","timestamp":1650946228969,"user_tz":-420,"elapsed":1524,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"003d6223-d0b9-43e7-8a4c-881b0cf5474c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",")"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["#### Inference\n","Xem notebook \"BLARimplement - SOBI-FromAll\""],"metadata":{"id":"6G6km6fmC4wQ"}}]}