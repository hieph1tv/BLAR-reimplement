{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_SOBI.ipynb","provenance":[],"collapsed_sections":["UZfgcsaOUugf","WIF8weFhoPBc","iYhcpncKeZGr","tNWSj-21o2Nk","tOve4EYtXmPV","U3zDogmTe5sB","ihFq4paU_k66","1I2sBB9u52-S"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d32f1f781b554b77a696622a97b253ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_292a9e2c5df3495bba2a9e1350f3f56b","IPY_MODEL_d959a309ddc743fcbddd911ca45578a1","IPY_MODEL_0d1574a07f274660b4d5a1399d6529a3"],"layout":"IPY_MODEL_dc09372cd57246e8bc431ecda312f525"}},"292a9e2c5df3495bba2a9e1350f3f56b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab963d0f948548418edd05b6ea65b82f","placeholder":"​","style":"IPY_MODEL_9b4ad0e59c224576850fa7a18997d251","value":"Downloading: 100%"}},"d959a309ddc743fcbddd911ca45578a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_377ab94289ef4231b5066ef242a343a1","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_420e3c566ce84f6aafa965c821b597c1","value":385}},"0d1574a07f274660b4d5a1399d6529a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12cd9df34a0e46b7944b6ad1164954fd","placeholder":"​","style":"IPY_MODEL_56e41db9eecc4aa8bb05fe47eac499c5","value":" 385/385 [00:00&lt;00:00, 13.2kB/s]"}},"dc09372cd57246e8bc431ecda312f525":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab963d0f948548418edd05b6ea65b82f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b4ad0e59c224576850fa7a18997d251":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"377ab94289ef4231b5066ef242a343a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"420e3c566ce84f6aafa965c821b597c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12cd9df34a0e46b7944b6ad1164954fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56e41db9eecc4aa8bb05fe47eac499c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7bf57554dc646a28c8a88ddb5722c35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d73be88b5224fa282191a98ca32ebca","IPY_MODEL_1531675690a445249c23cde01cc605ad","IPY_MODEL_6079b8c31a144b5b933aab71647c3fe1"],"layout":"IPY_MODEL_f89ed16e56f046dd8ff9cfdd2b023b7f"}},"8d73be88b5224fa282191a98ca32ebca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86c11aabc5ce45e883f8f634fa5be3ee","placeholder":"​","style":"IPY_MODEL_11e43e02bce34c3294d179b27fd7030e","value":"Downloading: 100%"}},"1531675690a445249c23cde01cc605ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84e86afb8703431daacede35dc0b5a75","max":222296,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75474b190a0e49ddb6e96e5160b6a08c","value":222296}},"6079b8c31a144b5b933aab71647c3fe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62e84c5ed0194bc38350a70cfb1f01cf","placeholder":"​","style":"IPY_MODEL_ec5c9b4f806b4e26aae82114f32870a2","value":" 217k/217k [00:00&lt;00:00, 934kB/s]"}},"f89ed16e56f046dd8ff9cfdd2b023b7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86c11aabc5ce45e883f8f634fa5be3ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11e43e02bce34c3294d179b27fd7030e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84e86afb8703431daacede35dc0b5a75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75474b190a0e49ddb6e96e5160b6a08c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62e84c5ed0194bc38350a70cfb1f01cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec5c9b4f806b4e26aae82114f32870a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3910b8de2b3540179b7a440fdba522ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_766c831ea8504f1ba0dd7d25c5a27ead","IPY_MODEL_b7ff55fe95994c6ba4d3b4dac67a9005","IPY_MODEL_c37d8c0908a74fd2a62fc7589bc4f29f"],"layout":"IPY_MODEL_3fd9a47202764dafad51df67fb0a27ea"}},"766c831ea8504f1ba0dd7d25c5a27ead":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dac4061627f40f3bc4eeb08ebd4e555","placeholder":"​","style":"IPY_MODEL_7c583ff2a4bc47fbaae71af2d789f4fb","value":"Downloading: 100%"}},"b7ff55fe95994c6ba4d3b4dac67a9005":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f43871a9fd8e47bcac40bda911e90d88","max":442301670,"min":0,"orientation":"horizontal","style":"IPY_MODEL_048be52e79e84198a485a3af73daa771","value":442301670}},"c37d8c0908a74fd2a62fc7589bc4f29f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b8c54832f2c4a02a4e9c56c58634650","placeholder":"​","style":"IPY_MODEL_2f35162253154fe4b1fcfa5b522449c1","value":" 422M/422M [00:12&lt;00:00, 42.0MB/s]"}},"3fd9a47202764dafad51df67fb0a27ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dac4061627f40f3bc4eeb08ebd4e555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c583ff2a4bc47fbaae71af2d789f4fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f43871a9fd8e47bcac40bda911e90d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"048be52e79e84198a485a3af73daa771":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b8c54832f2c4a02a4e9c56c58634650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f35162253154fe4b1fcfa5b522449c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Thông tin model:\n","scibert_based_cased\n","\n","Phân loại câu dựa trên phân loại token gán nhãn SOBI\n","max_len = 512\n","\n","\n"],"metadata":{"id":"zlbJdo4NNifh"}},{"cell_type":"markdown","source":["# Chuẩn bị các thư viện cần thiết"],"metadata":{"id":"UZfgcsaOUugf"}},{"cell_type":"markdown","source":["Cài đặt thư viện bioc và transformers"],"metadata":{"id":"mQE8ZQWs85S8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNgnmHf-MDG8","executionInfo":{"status":"ok","timestamp":1650944379186,"user_tz":-420,"elapsed":3595,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"991df8e0-5967-41fc-9843-bbbc6225942b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bioc in /usr/local/lib/python3.7/dist-packages (2.0.post4)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from bioc) (3.0.0)\n","Requirement already satisfied: intervaltree in /usr/local/lib/python3.7/dist-packages (from bioc) (2.1.0)\n","Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from bioc) (4.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bioc) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines>=1.2.0->bioc) (4.2.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines>=1.2.0->bioc) (21.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree->bioc) (2.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["pip install bioc transformers"]},{"cell_type":"code","source":["# import các thư viện\n","import os\n","import transformers\n","import bioc\n","import math\n","import torch\n","import pandas as pd\n","import numpy as np\n","from copy import deepcopy\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertForTokenClassification, AutoModelForTokenClassification\n","from sklearn.metrics import accuracy_score\n","from matplotlib import pyplot as plt"],"metadata":{"id":"ZCKoC-xHoPBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Kiểm tra colab sử dụng GPU để tăng tốc độ train\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944384980,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"5644b752-ea23-4224-f8d2-5e877071131c","id":"fVtQ0PlaoPBb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["# Import data"],"metadata":{"id":"WIF8weFhoPBc"}},{"cell_type":"code","source":["#@title Thực hiện import data bằng 1 trong 2 cách\n","#@markdown Cách 1: tải folder BLAR về google drive\n","\n","#@markdown Cách 2: huấn luyện thử mô hình, tải dữ liệu về ổ cứng tạm thời của colab\n","\n","method = \"download_to_temporary_disk\" #@param [\"download_to_google_drive\", \"download_to_temporary_disk\"]\n","\n","\n","if method == 'download_to_google_drive':\n","  \n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  os.chdir('/content/drive/MyDrive/Colab Notebooks/BLAR data') ##Đổi về directory trong drive\n","\n","else:\n","  os.chdir('/content')\n","  directory = './corpus'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 16hOCtaVyuE2LK_n7mHNO_ptm_A-A8kjV\n","  !gdown --id 1Mj80pavNCcIHB7_e59DVQyv0EoS8bI6a\n","  os.chdir('/content')\n","\n","  directory = './model/model_SOBI'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 1Xg7XBdIWs5k76KEB9J1pG8HikPsIdnKM\n","  !gdown --id 1rjDB8_GmTAvBrzu8qp7TfxORl0zIMm1b\n","  !gdown --id 1MvJ0Auu0l_XrjcM9prj0xj-E9ckMXO98\n","  os.chdir('/content')\n","\n","  directory = './model/2stepFromAllSentenceModel'\n","  if not os.path.exists(directory):\n","      os.makedirs(directory)\n","  os.chdir(directory)\n","  ## Download data về disk\n","  !gdown --id 1Lut7rci259_d3waHkdRjS3MGLwBN02Pe\n","  !gdown --id 1AIBtnfEYgJlmXFBzT0kZrEVnnBfyTjTL\n","  !gdown --id 10-U7cjGOiZ6Nty7OC3BCtyswP6ilk4VY\n","  os.chdir('/content')\n","\n","## Load data từ google drive\n","### BIOADI corpus\n","from bioc import biocxml\n","with open('./corpus/bioadi_bioc_gold.xml', 'r') as fp:\n","  gold_raw = biocxml.load(fp)\n","\n","### AB3P corpus\n","from bioc import biocxml\n","with open('./corpus/Ab3P_bioc_gold.xml', 'r') as fp:\n","  ab3p = biocxml.load(fp)\n"],"metadata":{"cellView":"form","id":"XYZEx23CVudP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650960510650,"user_tz":-420,"elapsed":24574,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"beeb6434-f3ec-4b13-efc9-782493075671"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=16hOCtaVyuE2LK_n7mHNO_ptm_A-A8kjV\n","To: /content/corpus/bioadi_bioc_gold.xml\n","100% 2.58M/2.58M [00:00<00:00, 232MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Mj80pavNCcIHB7_e59DVQyv0EoS8bI6a\n","To: /content/corpus/Ab3P_bioc_gold.xml\n","100% 2.36M/2.36M [00:00<00:00, 246MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Xg7XBdIWs5k76KEB9J1pG8HikPsIdnKM\n","To: /content/model/model_SOBI/config.json\n","100% 883/883 [00:00<00:00, 1.42MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1rjDB8_GmTAvBrzu8qp7TfxORl0zIMm1b\n","To: /content/model/model_SOBI/pytorch_model.bin\n","100% 437M/437M [00:01<00:00, 246MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1MvJ0Auu0l_XrjcM9prj0xj-E9ckMXO98\n","To: /content/model/model_SOBI/vocab.txt\n","100% 222k/222k [00:00<00:00, 105MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1Lut7rci259_d3waHkdRjS3MGLwBN02Pe\n","To: /content/model/2stepFromAllSentenceModel/config.json\n","100% 959/959 [00:00<00:00, 1.91MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1AIBtnfEYgJlmXFBzT0kZrEVnnBfyTjTL\n","To: /content/model/2stepFromAllSentenceModel/pytorch_model.bin\n","100% 437M/437M [00:01<00:00, 295MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=10-U7cjGOiZ6Nty7OC3BCtyswP6ilk4VY\n","To: /content/model/2stepFromAllSentenceModel/vocab.txt\n","100% 222k/222k [00:00<00:00, 70.8MB/s]\n"]}]},{"cell_type":"code","source":["# Tạo corpus phụ gồm các văn bản không chứa từ viết tắt\n","n = len(gold_raw.documents)\n","noAcronym = []\n","gold_noAcronym = bioc.bioc.BioCCollection()\n","for i, document in enumerate(gold_raw.documents):\n","  if len(document.passages[0].annotations) == 0:\n","    gold_noAcronym.add_document(document)\n","    noAcronym.append(i)"],"metadata":{"id":"9O77CO-QCsx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tạo corpus phụ gồm các văn bản chứa từ viết tắt\n","n = len(gold_raw.documents)\n","Acronym = []\n","gold = bioc.bioc.BioCCollection()\n","for i, document in enumerate(gold_raw.documents):\n","  if len(document.passages[0].annotations) != 0:\n","    gold.add_document(document)\n","    Acronym.append(i)"],"metadata":{"id":"NSwvknWig941"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load SciBERT model"],"metadata":{"id":"iYhcpncKeZGr"}},{"cell_type":"code","source":["tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n","scibert_model = transformers.AutoModel.from_pretrained('allenai/scibert_scivocab_cased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["d32f1f781b554b77a696622a97b253ca","292a9e2c5df3495bba2a9e1350f3f56b","d959a309ddc743fcbddd911ca45578a1","0d1574a07f274660b4d5a1399d6529a3","dc09372cd57246e8bc431ecda312f525","ab963d0f948548418edd05b6ea65b82f","9b4ad0e59c224576850fa7a18997d251","377ab94289ef4231b5066ef242a343a1","420e3c566ce84f6aafa965c821b597c1","12cd9df34a0e46b7944b6ad1164954fd","56e41db9eecc4aa8bb05fe47eac499c5","f7bf57554dc646a28c8a88ddb5722c35","8d73be88b5224fa282191a98ca32ebca","1531675690a445249c23cde01cc605ad","6079b8c31a144b5b933aab71647c3fe1","f89ed16e56f046dd8ff9cfdd2b023b7f","86c11aabc5ce45e883f8f634fa5be3ee","11e43e02bce34c3294d179b27fd7030e","84e86afb8703431daacede35dc0b5a75","75474b190a0e49ddb6e96e5160b6a08c","62e84c5ed0194bc38350a70cfb1f01cf","ec5c9b4f806b4e26aae82114f32870a2","3910b8de2b3540179b7a440fdba522ac","766c831ea8504f1ba0dd7d25c5a27ead","b7ff55fe95994c6ba4d3b4dac67a9005","c37d8c0908a74fd2a62fc7589bc4f29f","3fd9a47202764dafad51df67fb0a27ea","3dac4061627f40f3bc4eeb08ebd4e555","7c583ff2a4bc47fbaae71af2d789f4fb","f43871a9fd8e47bcac40bda911e90d88","048be52e79e84198a485a3af73daa771","2b8c54832f2c4a02a4e9c56c58634650","2f35162253154fe4b1fcfa5b522449c1"]},"id":"a70sTDySeYzr","executionInfo":{"status":"ok","timestamp":1650944680473,"user_tz":-420,"elapsed":19704,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"e9aebdbb-570e-4dca-a90a-4731c408735b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32f1f781b554b77a696622a97b253ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/217k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bf57554dc646a28c8a88ddb5722c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3910b8de2b3540179b7a440fdba522ac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# sample tokenization\n","sample = gold.documents[0].passages[0].text\n","encoding = tokenizer.encode(sample,\n","                            return_offsets_mapping=True, \n","                            padding='max_length', \n","                            truncation=True, \n","                            max_length=512)\n","\n","print(encoding)\n","print(tokenizer.convert_ids_to_tokens(encoding))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QudmTBS7jYR2","executionInfo":{"status":"ok","timestamp":1650944680474,"user_tz":-420,"elapsed":11,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"c20f6e33-39b5-403c-bc6b-72964945ae17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 105, 1174, 1291, 5839, 11986, 8589, 307, 864, 24820, 3254, 16737, 430, 2107, 1422, 136, 9297, 1004, 648, 211, 5839, 11986, 8589, 640, 231, 2771, 5922, 202, 1682, 201, 3683, 6990, 146, 8676, 111, 22330, 6746, 5839, 11986, 26164, 30111, 1820, 1185, 1066, 125, 5839, 11986, 26164, 30111, 231, 1109, 188, 105, 4035, 125, 3918, 211, 18221, 289, 2306, 111, 24820, 3254, 16737, 125, 105, 1291, 2475, 1538, 324, 7900, 5839, 11986, 26164, 578, 1174, 5839, 11986, 8589, 307, 143, 160, 22051, 10936, 30108, 551, 211, 111, 4097, 143, 262, 711, 134, 19304, 30108, 136, 825, 1066, 551, 163, 4434, 3055, 124, 4006, 430, 596, 188, 4712, 659, 21449, 211, 111, 160, 22051, 10936, 30108, 8881, 10369, 5839, 11986, 17511, 16539, 124, 711, 28880, 136, 1755, 23936, 30112, 3581, 430, 136, 9904, 4777, 15344, 10670, 136, 1487, 10409, 9615, 430, 596, 319, 21002, 2337, 211, 4237, 9297, 731, 3407, 202, 111, 160, 22051, 10936, 30108, 11050, 803, 146, 111, 1755, 23936, 30112, 1487, 136, 797, 146, 111, 4284, 1642, 201, 111, 1487, 211, 111, 9297, 1513, 1109, 188, 306, 1487, 10409, 9297, 476, 231, 1198, 430, 136, 305, 163, 7395, 202, 160, 22051, 10936, 30108, 441, 105, 1267, 4284, 2350, 2358, 136, 105, 1487, 3944, 2358, 211, 111, 4777, 15344, 10670, 1065, 163, 2002, 1158, 146, 111, 1511, 125, 4284, 6172, 211, 124, 2187, 146, 1280, 750, 578, 1174, 2708, 9376, 5922, 202, 5907, 5839, 11986, 8589, 307, 1088, 430, 160, 22051, 10936, 30108, 8881, 10369, 645, 5839, 11986, 26164, 30111, 136, 441, 537, 521, 700, 12342, 1088, 211, 125, 2914, 1650, 430, 160, 22051, 10936, 30108, 1484, 319, 1682, 201, 10097, 115, 663, 16539, 211, 125, 111, 1515, 5621, 2769, 430, 645, 4851, 508, 28958, 21757, 8225, 11986, 16521, 143, 1170, 21324, 551, 9668, 171, 136, 16794, 16018, 10144, 111, 12342, 1088, 211, 111, 3641, 224, 1170, 21324, 163, 2670, 188, 111, 9714, 2327, 1356, 173, 111, 4097, 578, 105, 16063, 2090, 1856, 30105, 188, 105, 9714, 1225, 223, 5769, 125, 958, 578, 16463, 430, 193, 30119, 578, 23174, 136, 2311, 578, 23199, 211, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","['[CLS]', 'a', 'specific', 'human', 'lys', '##ophosph', '##olip', '##ase', ':', 'cd', '##na', 'cloning', ',', 'tissue', 'distribution', 'and', 'kinetic', 'character', '##ization', '.', 'lys', '##ophosph', '##olip', '##ases', 'are', 'critical', 'enzymes', 'that', 'act', 'on', 'biological', 'membranes', 'to', 'regulate', 'the', 'multif', '##unctional', 'lys', '##ophosph', '##olipid', '##s', ';', 'increased', 'levels', 'of', 'lys', '##ophosph', '##olipid', '##s', 'are', 'associated', 'with', 'a', 'host', 'of', 'diseases', '.', 'herein', 'we', 'report', 'the', 'cd', '##na', 'cloning', 'of', 'a', 'human', 'brain', '25', 'k', '##da', 'lys', '##ophosph', '##olipid', '-', 'specific', 'lys', '##ophosph', '##olip', '##ase', '(', 'h', '##lys', '##opl', '##a', ')', '.', 'the', 'enzyme', '(', 'at', 'both', 'm', '##rn', '##a', 'and', 'protein', 'levels', ')', 'is', 'widely', 'distributed', 'in', 'tissues', ',', 'but', 'with', 'quite', 'different', 'abundances', '.', 'the', 'h', '##lys', '##opl', '##a', 'hydroly', '##zes', 'lys', '##ophosph', '##atidyl', '##choline', 'in', 'both', 'monomeric', 'and', 'mice', '##lla', '##r', 'forms', ',', 'and', 'exhibits', 'apparent', 'cooper', '##ativity', 'and', 'surface', 'dilution', 'kinetics', ',', 'but', 'not', 'interfacial', 'activation', '.', 'detailed', 'kinetic', 'analysis', 'indicates', 'that', 'the', 'h', '##lys', '##opl', '##a', 'binds', 'first', 'to', 'the', 'mice', '##lla', '##r', 'surface', 'and', 'then', 'to', 'the', 'substrate', 'presented', 'on', 'the', 'surface', '.', 'the', 'kinetic', 'parameters', 'associated', 'with', 'this', 'surface', 'dilution', 'kinetic', 'model', 'are', 'reported', ',', 'and', 'it', 'is', 'concluded', 'that', 'h', '##lys', '##opl', '##a', 'has', 'a', 'single', 'substrate', 'binding', 'site', 'and', 'a', 'surface', 'recognition', 'site', '.', 'the', 'apparent', 'cooper', '##ativity', 'observed', 'is', 'likely', 'due', 'to', 'the', 'change', 'of', 'substrate', 'presentation', '.', 'in', 'contrast', 'to', 'many', 'non', '-', 'specific', 'lip', '##olytic', 'enzymes', 'that', 'exhibit', 'lys', '##ophosph', '##olip', '##ase', 'activity', ',', 'h', '##lys', '##opl', '##a', 'hydroly', '##zes', 'only', 'lys', '##ophosph', '##olipid', '##s', 'and', 'has', 'no', 'other', 'significant', 'enzymatic', 'activity', '.', 'of', 'special', 'interest', ',', 'h', '##lys', '##opl', '##a', 'does', 'not', 'act', 'on', 'plasm', '##en', '##yl', '##choline', '.', 'of', 'the', 'several', 'inhibitors', 'tested', ',', 'only', 'methyl', 'ar', '##achid', '##onyl', 'fluor', '##ophosph', '##onate', '(', 'ma', '##fp', ')', 'potent', '##ly', 'and', 'irrever', '##sibly', 'inhibits', 'the', 'enzymatic', 'activity', '.', 'the', 'inhibition', 'by', 'ma', '##fp', 'is', 'consistent', 'with', 'the', 'catalytic', 'mechanism', 'proposed', 'for', 'the', 'enzyme', '-', 'a', 'serine', 'hydro', '##las', '##e', 'with', 'a', 'catalytic', 'tri', '##ad', 'composed', 'of', 'ser', '-', '119', ',', 'as', '##p', '-', '174', 'and', 'his', '-', '208', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"]}]},{"cell_type":"markdown","source":["# Sentence classification model"],"metadata":{"id":"EnqeGxt43otb"}},{"cell_type":"code","source":["MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 4\n","EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"metadata":{"id":"-TRV1hQWVWdI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"cO9NTJIe3rif"}},{"cell_type":"code","source":["#@title functions\n","def BIOTagSequenceWithAcronym(original_passage):\n","  # this function will return list of token's labels\n","  passage = deepcopy(original_passage)\n","  text = passage.text\n","  # extract annotation\n","  sf_offset_stack = []\n","  for i, annotation in enumerate(passage.annotations):\n","    if i%2 == 0:\n","      sf_offset_stack.append([i,passage.annotations[i].total_span.offset]) #extract the SF offset only\n","  \n","  # Sentence extraction\n","  ## Presumption: \n","  ### SF always stands before LF in corpus annotation list\n","  ### Sentence extraction by: '. '\n","  ### Sentence starts with CASED character\n","  ### Sentence length > 20 characters\n","  rawSentenceList = text.split('. ') #split passage to list of sentence\n","  \n","  cased_alphabet = 'ABCEDEFGHIJKLMNOPQRSTUVWXYZ'\n","  sentenceList = []\n","  for i, raw_sentence in enumerate(rawSentenceList):\n","    if raw_sentence[-1] != '.':\n","      raw_sentence += '.' # add . if sentence doesn't have\n","    if raw_sentence[0] in cased_alphabet and len(raw_sentence) >20:\n","      sentenceList.append(raw_sentence)\n","    else:\n","      if sentenceList:\n","        sentenceList[-1] += ' ' + raw_sentence\n","      else:\n","        sentenceList.append(raw_sentence)\n","  \n","  # add annotation for sentence\n","  pointer = 0\n","  sentenceTag = ['NOTcontainSFLF']*len(sentenceList)\n","  for i,sentence in enumerate(sentenceList):\n","    while len(sf_offset_stack):\n","      if sf_offset_stack[0][1] < pointer + len(sentence):\n","        sf = sf_offset_stack.pop(0)\n","        sentenceTag[i]= 'containSFLF'  \n","      else:    \n","        break\n","    \n","    pointer += len(sentence) + 1 # move pointer to next sentence, 1 for .\n","\n","\n","\n","  wordTag = []\n","  for i,sentence in enumerate(sentenceList):\n","    sentence_wordlength = len(sentence.split(' '))\n","    if sentenceTag[i] == 'containSFLF':\n","      wordTag += 'B'\n","      wordTag += ['I']*(sentence_wordlength-1)\n","    elif sentenceTag[i] == 'NOTcontainSFLF':\n","      wordTag += 'S'\n","      wordTag += ['O']*(sentence_wordlength-1)\n","  return wordTag\n","        "],"metadata":{"id":"V327DccD3ojt","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create tagged sentence corpus\n","sentence_corpus = []\n","tag_corpus = []\n","\n","for i,document in enumerate(gold_raw.documents):\n","  sentence_corpus.append(document.passages[0].text)\n","  tag_corpus.append(BIOTagSequenceWithAcronym(document.passages[0]))\n","sentence_df = pd.DataFrame({'sentence':sentence_corpus,'labels':tag_corpus})"],"metadata":{"id":"LcTskIdrQkHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot histogram of passage length\n","y = [pair[1] for pair in [(i,len(sentence.split(' '))) for i,sentence in enumerate(sentence_df.sentence.values)]] ##word length\n","plt.hist(y)"],"metadata":{"id":"Y_9LfY2p3oWJ","colab":{"base_uri":"https://localhost:8080/","height":338},"executionInfo":{"status":"ok","timestamp":1650944719339,"user_tz":-420,"elapsed":558,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"1b6dcafd-dfd3-4a9b-fa9a-0982affddd47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 75., 299., 309., 302., 151.,  50.,  12.,   2.,   0.,   1.]),\n"," array([ 78. , 121.6, 165.2, 208.8, 252.4, 296. , 339.6, 383.2, 426.8,\n","        470.4, 514. ]),\n"," <a list of 10 Patch objects>)"]},"metadata":{},"execution_count":25},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# label dict for SOBI model\n","labels_to_ids_sentence = {'B':0,'I':1,'S':2, 'O':3,'SpecialToken':-100} #'O':3,\n","ids_to_labels_sentence = {0:'B',1:'I',2:'S', 3: 'O', -100:'SpecialToken'} # 3: 'O',"],"metadata":{"id":"4mOaa9uJAGly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SequenceClassification Model"],"metadata":{"id":"Wue0MA8ONF8a"}},{"cell_type":"markdown","source":["### Preparation"],"metadata":{"id":"tNWSj-21o2Nk"}},{"cell_type":"code","source":["# load pretrained model with new output length\n","model_seq = AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_cased', num_labels=len(ids_to_labels_sentence.items()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u37tY_gP3oTo","executionInfo":{"status":"ok","timestamp":1650944721420,"user_tz":-420,"elapsed":2086,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"8154b0e6-97f1-42b9-914f-6acd9aef17a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["#@title create dataset class\n","class dataset_seq(Dataset):\n","  def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","  def __getitem__(self, index):\n","        # step 1: get the sentence and word labels \n","        text = self.data.sentence[index].split(' ')\n","        sentence_label = self.data.labels[index]\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","\n","\n","        encoding = self.tokenizer(text,\n","                             is_split_into_words=True, \n","                             return_offsets_mapping=True, \n","                             padding='max_length', \n","                             truncation=True, \n","                             max_length=self.max_len)        \n","       \n","\n","        # step 3: create token labels only for first word pieces of each tokenized word\n","        labels = [labels_to_ids_sentence[label] for label in sentence_label] \n","\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            i += 1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","       \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"metadata":{"id":"FVrXFqCw3oRB","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train test split\n","train_size = 0.8\n","train_dataset_seq = sentence_df.sample(frac=train_size,random_state=200)\n","test_dataset_seq = sentence_df.drop(train_dataset_seq.index).reset_index(drop=True)\n","train_dataset_seq = train_dataset_seq.reset_index(drop=True)\n","\n","print(\"FULL Dataset sequence: {}\".format(sentence_df.shape))\n","print(\"TRAIN Dataset sequence: {}\".format(train_dataset_seq.shape))\n","print(\"TEST Dataset sequence: {}\".format(test_dataset_seq.shape))\n","\n","training_set = dataset_seq(train_dataset_seq, tokenizer, MAX_LEN)\n","testing_set = dataset_seq(test_dataset_seq, tokenizer, MAX_LEN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCzr1MojPQO-","executionInfo":{"status":"ok","timestamp":1650944721422,"user_tz":-420,"elapsed":13,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"aeeadc67-8c94-486e-b45d-a7fbd0635cf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FULL Dataset sequence: (1201, 2)\n","TRAIN Dataset sequence: (961, 2)\n","TEST Dataset sequence: (240, 2)\n"]}]},{"cell_type":"code","source":["# create dataloader\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"metadata":{"id":"pDmZNRykP1na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move to GPU\n","model_seq.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944732295,"user_tz":-420,"elapsed":10883,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"fb4a9f60-e4cb-4017-fd28-183e85061ff2","id":"i8jJ6wrtQcyd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Sample input"],"metadata":{"id":"GZiB7Z53Qcyj"}},{"cell_type":"code","source":["inputs = training_set[5]\n","input_ids = inputs[\"input_ids\"].unsqueeze(0)\n","attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n","labels = inputs[\"labels\"].unsqueeze(0)\n","\n","input_ids = input_ids.to(device)\n","attention_mask = attention_mask.to(device)\n","labels = labels.to(device)\n","\n","outputs = model_seq(input_ids, attention_mask=attention_mask, labels = labels)\n","\n","print(f'loss: {outputs[0]}, logit {outputs[1]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944732296,"user_tz":-420,"elapsed":24,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"42660433-d706-488a-cb57-52bea92fd6fc","id":"Vzj4nwEbQcyj"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss: 1.6534124612808228, logit tensor([[[-0.0768, -0.7454,  0.1025, -0.0455, -0.2533],\n","         [-0.0038, -0.7652,  0.0726, -0.2042, -0.1944],\n","         [-0.5929, -0.9600,  0.9215, -0.6778,  0.1617],\n","         ...,\n","         [-0.7652, -0.8520,  0.5033,  0.1598,  0.0505],\n","         [-0.3632, -0.2640,  0.1006, -0.0555, -0.1786],\n","         [-0.3974, -0.2953,  0.1300,  0.2542, -0.4024]]], device='cuda:0',\n","       grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["# random prediction logit\n","-math.log(1/5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gm3dbCIVKTf","executionInfo":{"status":"ok","timestamp":1650944732297,"user_tz":-420,"elapsed":21,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"9e553e79-e54b-45de-dfc1-499e74de9e7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.6094379124341003"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["### Training\n","Thực hiện cell này để train lại model"],"metadata":{"id":"VRfvPyc0Qcyj"}},{"cell_type":"code","source":["#@ Optimizer\n","LEARNING_RATE = 1e-5 #@param {type:\"number\"}\n","optimizer = 'adam' #@param{type:\"string\"}\n","optimizer = torch.optim.Adam(params=model_seq.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"DPB5EjkqQcyj","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title define traning function\n","# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model_seq.train()\n","    \n","    for idx, batch in enumerate(training_loader):\n","  \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        outputs = model_seq(input_ids=ids, attention_mask=mask, labels=labels)\n","        \n","        tr_logits = outputs[1]\n","        loss = outputs[0]\n","\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 10==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss at training step {idx}: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model_seq.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != 3 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n"," \n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model_seq.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")"],"metadata":{"id":"_XsGaE56Qcyj","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title train\n","EPOCHS = 5 #@param {type:\"integer\"}\n","for epoch in range(EPOCHS):\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b06a8d66-99d3-48eb-942f-7a7d02fd4c27","id":"WKebb0oMQcyj","executionInfo":{"status":"error","timestamp":1650944897103,"user_tz":-420,"elapsed":164825,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss at training step 0: 1.6772611141204834\n","Training loss at training step 10: 0.9580720663070679\n","Training loss at training step 20: 0.803298647914614\n","Training loss at training step 30: 0.7179690734032662\n","Training loss at training step 40: 0.657503357747706\n","Training loss at training step 50: 0.620368700985815\n","Training loss at training step 60: 0.586518301338446\n","Training loss at training step 70: 0.5578768257523926\n","Training loss at training step 80: 0.5276004600304144\n","Training loss at training step 90: 0.5009553082041688\n","Training loss at training step 100: 0.47036667441082475\n","Training loss at training step 110: 0.44559639537925116\n","Training loss at training step 120: 0.4217986191107222\n","Training loss epoch: 0.4217986191107222\n","Training accuracy epoch: 0.039534845849854804\n","Training epoch: 2\n","Training loss at training step 0: 0.24017764627933502\n","Training loss at training step 10: 0.15277546813542192\n","Training loss at training step 20: 0.14668730930203483\n","Training loss at training step 30: 0.13178843175691943\n","Training loss at training step 40: 0.1232130555663167\n","Training loss at training step 50: 0.12025655988676875\n","Training loss at training step 60: 0.11570841361020433\n","Training loss at training step 70: 0.11527096800191301\n","Training loss at training step 80: 0.1138548792604311\n","Training loss at training step 90: 0.11221678497699591\n","Training loss at training step 100: 0.10913994067376202\n","Training loss at training step 110: 0.10868011242164685\n","Training loss at training step 120: 0.10802294789574855\n","Training loss epoch: 0.10802294789574855\n","Training accuracy epoch: 0.09818219335706303\n","Training epoch: 3\n","Training loss at training step 0: 0.05442862957715988\n","Training loss at training step 10: 0.06838033483787016\n","Training loss at training step 20: 0.05914290009864739\n","Training loss at training step 30: 0.060583504397542245\n","Training loss at training step 40: 0.06725602437991922\n","Training loss at training step 50: 0.07429736564118489\n","Training loss at training step 60: 0.07532058109636189\n","Training loss at training step 70: 0.0721881242705063\n","Training loss at training step 80: 0.06993932313757178\n","Training loss at training step 90: 0.06863465370958323\n","Training loss at training step 100: 0.06769435614863835\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-2c0d71c29c67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-b09f6a73fd84>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         torch.nn.utils.clip_grad_norm_(\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         raise RuntimeError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#@title evaluation\n","def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            \n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            \n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels_sentence[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels_sentence[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")\n","\n","    return labels, predictions\n","labels, predictions = valid(model_seq, testing_loader)"],"metadata":{"id":"sSnivibmQcyj","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944909709,"user_tz":-420,"elapsed":6681,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"a401c056-209b-43a9-e4b4-3e5172c562de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss per 100 evaluation steps: 0.020887425169348717\n","Validation Loss: 0.10246436722227373\n","Validation Accuracy: 0.9715505895984153\n"]}]},{"cell_type":"markdown","source":["### Save model"],"metadata":{"id":"tOve4EYtXmPV"}},{"cell_type":"code","source":["directory = \"./model/newsentencemodel\" # chọn tên model save lại\n","\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model_seq.save_pretrained(directory)\n","print('All files saved')"],"metadata":{"executionInfo":{"status":"ok","timestamp":1650944922896,"user_tz":-420,"elapsed":1146,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"id":"nN4NU0iPXD4g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a51d261d-aee1-4e23-df8f-417d8c7f278d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All files saved\n"]}]},{"cell_type":"markdown","source":["# Model inference\n","Thực hiện trên model cũ hoặc model mới train"],"metadata":{"id":"WWRUAs-jbXf8"}},{"cell_type":"markdown","source":["### Load model"],"metadata":{"id":"U3zDogmTe5sB"}},{"cell_type":"code","source":["#@title Reload functions\n","#@markdown (use when not train, infer old model)\n","def ExtractSentenceWithAcronym(original_passage):\n","  # this function will return new passage with annotated sentences\n","  passage = deepcopy(original_passage)\n","  text = passage.text\n","  # extract annotation\n","  sf_offset_stack_unsort = dict()\n","  for i, annotation in enumerate(passage.annotations):\n","    if i%2 == 0:\n","      sf_offset_stack_unsort[passage.annotations[i].total_span.offset] = i #extract the SF offset only\n","  offsetList = [offset for offset,i in sf_offset_stack_unsort.items()]\n","  \n","  sf_offset_stack = []\n","  while offsetList:\n","\n","    minoffset = min(offsetList)\n","    idx = sf_offset_stack_unsort[minoffset]\n","    sf_offset_stack.append([idx, passage.annotations[idx].total_span.offset])\n","    offsetList.remove(minoffset)\n","\n","    # sentence extraction\n","  rawSentenceList = text.split('. ') #split passage to list of sentence\n","  \n","  cased_alphabet = 'ABCEDEFGHIJKLMNOPQRSTUVWXYZ'\n","  sentenceList = []\n","  for i, raw_sentence in enumerate(rawSentenceList):\n","    if raw_sentence[-1] != '.':\n","      raw_sentence += '.' # add . if sentence doesn't have\n","    if raw_sentence[0] in cased_alphabet and len(raw_sentence) >10:\n","      sentenceList.append(raw_sentence)\n","    else:\n","      if sentenceList:\n","        sentenceList[-1] += ' ' + raw_sentence\n","      else:\n","        sentenceList.append(raw_sentence)\n","\n","  # add annotation for sentence\n","  pointer = 0\n","  for sentence in sentenceList:\n","    bioc_sentence = bioc.bioc.BioCSentence() \n","    bioc_sentence.text = sentence\n","    while len(sf_offset_stack):\n","      if sf_offset_stack[0][1] < pointer + len(sentence):\n","        sf = sf_offset_stack.pop(0)\n","        bioc_sentence.annotations.append(passage.annotations[sf[0]]) # get SF annotation\n","        bioc_sentence.annotations.append(passage.annotations[sf[0]+1]) # get LF annotation\n","        bioc_sentence.annotations[-2].locations[0].offset -= pointer # change SF offset to sentence (originally was offset passage)\n","        bioc_sentence.annotations[-1].locations[0].offset -= pointer # change LF offset to sentence (originally was offset passage)  \n","      else:\n","        break\n","    bioc_sentence.offset = pointer\n","    pointer += len(sentence) + 1 # move pointer to next sentence, 1 for .\n","    passage.add_sentence(bioc_sentence) # add sentence to passage\n","      \n","  return passage\n","\n","\n","def PredictToken(text):\n","  text = text.split(' ')\n","  inputs = tokenizer(text,\n","                    is_split_into_words=True,\n","                    return_offsets_mapping=True, \n","                    padding='max_length', \n","                    truncation=True, \n","                    max_length=512,\n","                    return_tensors=\"pt\")\n","\n","  # move to gpu\n","  ids = inputs[\"input_ids\"].to(device)\n","  mask = inputs[\"attention_mask\"].to(device)\n","  # forward pass\n","  outputs = model_seq(ids, attention_mask=mask)\n","  logits = outputs[0]\n","\n","  active_logits = logits.view(-1, model_seq.num_labels) # shape (batch_size * seq_len, num_labels)\n","  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","  tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","  token_predictions = [ids_to_labels_sentence[i] for i in flattened_predictions.cpu().numpy()]\n","  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","  prediction = []\n","  for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n","    #only predictions on first word pieces are important\n","    if mapping[0] == 0 and mapping[1] != 0:\n","      prediction.append(token_pred[1])\n","    else:\n","      continue\n","  return prediction\n","\n","def PredictSentenceWithABBR(text):\n","  text_split = text.split(' ')\n","  prediction = PredictToken(text)\n","  sentenceList = []\n","  for i in range(len(prediction)-1):\n","    if prediction[i] == 'B':\n","      for j in range(i+1, len(prediction)):\n","        if prediction[j] in 'BS':\n","          sentence = ' '.join(text_split[i:j])\n","          if j-i>4: # at least 5 words per sentence\n","            sentenceList.append(sentence)\n","          break\n","        elif j == len(prediction)-1:\n","          sentence = ' '.join(text_split[i:])\n","          sentenceList.append(sentence)\n","          break               \n","  return sentenceList\n","\n","def PredictBiocSentenceWithABBR(passage):\n","  sentences_from_passage = ExtractSentenceWithAcronym(passage).sentences\n","  sentenceList = PredictSentenceWithABBR(passage.text)\n","  biocSentenceList = []\n","  for sentence in sentences_from_passage:\n","    for i, annotation in enumerate(sentence.annotations):\n","      if i%2 == 0 :\n","        haveSF = sentence.annotations[0].text in ' '.join(PredictSentenceWithABBR(passage.text))\n","\n","        haveLF = []\n","        for word in sentence.annotations[1].text.split(' '):\n","          if word in ' '.join(PredictSentenceWithABBR(passage.text)):\n","            haveLF.append(1)\n","          else:\n","            haveLF.append(0)\n","        haveLF = bool(np.prod(haveLF))\n","        if haveSF and haveLF:\n","          if sentence not in biocSentenceList:\n","            biocSentenceList.append(sentence)\n","    if not sentence.annotations:\n","      if sentence.text in sentenceList:\n","        biocSentenceList.append(sentence)\n","      else:\n","        for predicted_sentence in sentenceList:\n","          if predicted_sentence in sentence.text:\n","            biocSentenceList.append(sentence)\n","  return biocSentenceList"],"metadata":{"id":"MaZMzHxpfPwH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory = \"./model/newsentencemodel\" # chọn model\n","tokenizer = transformers.AutoTokenizer.from_pretrained(directory, local_files_only=True)\n","model_seq = transformers.AutoModelForTokenClassification.from_pretrained(directory, local_files_only=True)\n","model_seq.to(device)"],"metadata":{"id":"W7K3RgKLe5E4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944927489,"user_tz":-420,"elapsed":1367,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"21c97b69-540c-4b3f-b72f-dca112c24505"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31116, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### check difference of SentenceModel"],"metadata":{"id":"ihFq4paU_k66"}},{"cell_type":"code","source":["#@title Create dataframe\n","difference = []\n","i = 0\n","for document in gold_raw.documents:\n","  passage = document.passages[0]\n","  count_true = len([sentence for sentence in ExtractSentenceWithAcronym(passage).sentences if sentence.annotations])\n","  count_predict = len(PredictBiocSentenceWithABBR(passage))\n","\n","  difference.append(count_true - count_predict)\n","\n","  if i%50 == 0:\n","    print(f'process passage {i}')\n","  i += 1"],"metadata":{"id":"QI1YRmFVilVc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650728884097,"user_tz":-420,"elapsed":306401,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"596e9986-5aa4-4019-acb8-90b7cd6a3ed5","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["process passage 0\n","process passage 50\n","process passage 100\n","process passage 150\n","process passage 200\n","process passage 250\n","process passage 300\n","process passage 350\n","process passage 400\n","process passage 450\n","process passage 500\n","process passage 550\n","process passage 600\n","process passage 650\n","process passage 700\n","process passage 750\n","process passage 800\n","process passage 850\n","process passage 900\n","process passage 950\n","process passage 1000\n","process passage 1050\n","process passage 1100\n","process passage 1150\n","process passage 1200\n"]}]},{"cell_type":"code","source":["#@title Create wrong prediction count table\n","table = []\n","for i,diff in enumerate(difference):\n","  if diff != 0:\n","    table.append((i,diff))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yeLvAq1lLuLZ","executionInfo":{"status":"ok","timestamp":1650728884098,"user_tz":-420,"elapsed":26,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"f9740b52-0b7a-4bb3-ee56-95396256b1b6","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(27, 1),\n"," (51, 2),\n"," (59, 2),\n"," (61, 1),\n"," (81, 1),\n"," (97, 1),\n"," (110, 1),\n"," (247, 1),\n"," (269, 1),\n"," (275, 1),\n"," (287, 1),\n"," (333, 1),\n"," (351, 1),\n"," (358, 1),\n"," (365, 1),\n"," (367, 1),\n"," (457, 1),\n"," (483, 1),\n"," (512, 1),\n"," (538, 1),\n"," (550, 1),\n"," (559, 1),\n"," (601, 1),\n"," (630, 1),\n"," (638, 1),\n"," (642, 1),\n"," (652, 2),\n"," (698, 1),\n"," (709, 2),\n"," (728, 1),\n"," (729, 1),\n"," (758, 1),\n"," (761, 1),\n"," (771, 1),\n"," (796, 1),\n"," (799, 1),\n"," (800, 1),\n"," (835, 1),\n"," (862, 1),\n"," (895, 1),\n"," (903, 1),\n"," (910, 1),\n"," (947, 2),\n"," (960, 1),\n"," (973, 1),\n"," (981, 1),\n"," (1004, 1),\n"," (1024, 1),\n"," (1091, 1),\n"," (1099, 1),\n"," (1110, 1),\n"," (1115, 1),\n"," (1138, 1),\n"," (1186, 1),\n"," (1190, 1),\n"," (1200, 1)]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["#@title Print false positive cases\n","table = []\n","for i,diff in enumerate(difference):\n","  if diff < 0:\n","    table.append((i,diff))\n","print(f'FP: {len(table)}')\n","for pair in table:\n","  idx = pair[0]\n","  passage = gold_raw.documents[idx].passages[0]\n","  for sentence in PredictBiocSentenceWithABBR(passage):\n","    if not sentence.annotations:\n","      print(sentence.text)"],"metadata":{"id":"xY_LKTHTzK5e","executionInfo":{"status":"ok","timestamp":1650728914561,"user_tz":-420,"elapsed":2474,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1f750be-781d-4ccc-caa1-68a9136a48a5","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FP: 19\n","Although cell surface receptors for the Fc region of IgA (Fc alpha R) have been implicated in a variety of immune effector mechanisms, the molecular features of Fc alpha R remain only marginally characterized.\n","Primary structure of the human melanoma-associated antigen p97 (melanotransferrin) deduced from the mRNA sequence. p97 is a cell-surface glycoprotein that is present in most human melanomas but only in trace amounts in normal adult tissues.\n","A full-length 1966-base pair clone of the human class IV alcohol dehydrogenase (sigma-ADH) was isolated from a human stomach cDNA library.\n","There are important amino acid differences in the alcohol-binding site between the human class IV (sigma) and human class I (beta) alcohol dehydrogenases that appear to explain the high catalytic efficiency for all-trans-retinol, the high kcat for ethanol, and the low catalytic efficiency for secondary alcohols of sigma-ADH relative to beta 1-ADH.\n","The beta-subunit of the high-affinity IgE receptor (Fc epsilon RI-beta) on chromosome 11 is maternally linked to atopy, the state of enhanced IgE responsiveness underlying allergic asthma and rhinitis.\n","Using a polymerase chain reaction based differential screening approach, we have isolated and characterised a cDNA from a human metastatic breast tumour representing a novel protein tyrosine kinase (brk).\n","Cloning, sequencing and expression of a novel cDNA encoding human vacuolar ATPase (14-kDa subunit).\n","The SCA8 transcript is an antisense RNA to a brain-specific transcript encoding a novel actin-binding protein (KLHL1).\n","We isolated cDNA encoding a novel fibroblast growth factor (FGF-22) (170 amino acids) from human placenta.\n","Human aminopeptidase B (rnpep) on chromosome 1q32.2: complementary DNA, genomic structure and expression.\n","The genomic structure of the human APB gene (rnpep; 1q32.1-q32.2) was also determined. rnpep is bracketed by pre-protein translocase of the inner mitochondrial membrane gene and ETS family transcription factor ELF3 gene.\n","To this end human embryonic kidney (HEK293) cells were transfected with the kinases and ion-channel activity determined using the patch-clamp technique.\n","Serological studies of 1705 sera from patients with suspected paraneoplastic neurological disorders resulted in the identification of four patients with antibodies that reacted with 37 and 40 kDa neuronal proteins (anti-Ma antibodies).\n","Probing of a human complementary DNA library with anti-Ma serum resulted in the cloning of a gene that encodes a novel 37 kDa protein (Mal).\n","2E4 (kaptin): a novel actin-associated protein from human blood platelets found in lamellipodia and the tips of the stereocilia of the inner ear.\n","NECA, in parallel with the activation of ERK, also stimulated the p46 isoform of c-Jun N-terminal kinase (MEK) and p38 MAPK.\n","Diadenosine oligophosphates (Ap(n)A) have been proposed as intracellular and extracellular signaling molecules in animal cells.\n","Ras proteins are a family of guanine nucleotide (GDP and GTP)-binding proteins that play central roles in essential signal transduction pathways.\n","Molecular cloning and expression of mouse GD1alpha/GT1aalpha/GQ1balpha synthase (ST6GalNAc VI) gene.\n","A novel member of the mouse CMP-NeuAc:beta-N-acetylgalactosaminide alpha2,6-sialyltransferase (ST6GalNAc) subfamily, designated ST6GalNAc VI, was identified by BLAST analysis of expressed sequence tags.\n","The complete cDNA sequence of human betaV spectrin is available from GenBank(TM) as accession number.\n","The COQ3 gene in Saccharomyces cerevisiae encodes an O-methyltransferase required for two steps in the biosynthetic pathway of ubiquinone (coenzyme Q, or Q).\n","Caspase-1 (interleukin-1beta converting enzyme) is produced in the form of a latent precursor, which is cleaved to yield a prodomain in addition to the p20 and p10 subunits.\n"]}]},{"cell_type":"code","source":["#@title Print false negative cases\n","table = []\n","for i,diff in enumerate(difference):\n","  if diff > 0:\n","    table.append((i,diff))\n","table\n","print(f'FN: {len(table)}')\n","\n","for pair in table:\n","  idx = pair[0]\n","  passage = gold_raw.documents[idx].passages[0]\n","  truesentence = [sentence for sentence in ExtractSentenceWithAcronym(passage).sentences if sentence.annotations]\n","  prediction = [sentence.text for sentence in PredictBiocSentenceWithABBR(passage)]\n","\n","  for sentence in truesentence:\n","  \n","    if sentence.text not in prediction and sentence.annotations:\n","      print(sentence.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7U_kB6I3NwaK","executionInfo":{"status":"ok","timestamp":1650728911443,"user_tz":-420,"elapsed":27356,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"29b4882f-4714-467f-c939-dbe8cd761a7b","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FN: 56\n","In comparison, homologues of the Rad23 proteins (hHR23) and the hXPC protein are involved in the recognition of damaged bases in global genome repair, a subset of nucleotide excision repair.\n","AMP-activated protein kinase (AMPK) is viewed as an energy sensor that acts to modulate glucose uptake and fatty acid oxidation in skeletal muscle.\n","Injections of 5-aminoimidazole-4-carboxamide 1-beta-d-ribonucleoside (AICAR) were used to activate AMPK in male rats.\n","The Ah (dioxin) receptor binds a number of widely disseminated environmental pollutants, including 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD) and polycyclic aromatic hydrocarbons, and mediates their carcinogenic effects.\n","The ligand-bound receptor activates Cyp 1a1 gene transcription through interaction with specific DNA sequences, termed xenobiotic responsive elements (XREs).\n","A cDNA encoding UDP-GlcNAc:Gal beta 1-3GalNAc-R (GlcNAc to GalNAc) beta 1-6GlcNAc transferase (EC 2.4.1.102), which forms critical branches in O-glycans, has been isolated by an expression cloning approach using Chinese hamster ovary (CHO) cells.\n","An antisense oligodeoxynucleotide designed to interrupt CHED's expression (AS-CHED) significantly reduced the ratio between CHED mRNA and actin mRNA within 1 hr of its addition to cultures, a reduction that persisted for 4 days.\n","Both rac1 transcripts (2.4 and 1.1 kilobases (kb] increase when HL-60 cells differentiate to neutrophil-like morphology.\n","Pregnancy-specific beta 1-glycoprotein (PS beta G), a major product of the placenta with unknown function, consists of a set of glycoproteins synthesized by the syncytiotrophoblast.\n","Studies of transcriptional activation by interferons and a variety of cytokines have led to the identification of a family of proteins that serve as signal transducers and activators of transcription (STAT).\n","The breast cancer specific tumour suppressor protein, BRCA1 (refs 1,2), activates transcription when linked with a DNA-binding domain and is a component of the RNA polymerase II (Pol II) holoenzyme.\n","Identification of a principal mRNA species for human 3alpha-hydroxysteroid dehydrogenase isoform (AKR1C3) that exhibits high prostaglandin D2 11-ketoreductase activity.\n","Binding to ligands induces conformational changes in the nuclear receptors that enable the receptors to interact with several types of cofactor that are critical for transcription activation (transactivation).\n","For example, the TEL1/ETV6 (TEL1) gene is required for normal yolk sac angiogenesis, adult bone marrow hematopoiesis, and is rearranged or deleted in numerous leukemias.\n","IL-10-related T cell-derived inducible factor (IL-TIF or IL-21) is a new cytokine structurally related to IL-10 and originally identified in the mouse as a gene induced by IL-9 in T cells and mast cells.\n","The orthologous human and murine semaphorin 6A-1 proteins (SEMA6A-1/Sema6A-1) bind to the enabled/vasodilator-stimulated phosphoprotein-like protein (EVL) via a novel carboxyl-terminal zyxin-like domain.\n","A recently identified membrane-type 6 matrix metalloproteinase (MT6-MMP) has a hydrophobic stretch of 24 amino acids at the C-terminus.\n","TASK-3 is inhibited by extracellular acidosis with a mid-point of inhibition around pH 6. 5, supporting the predictions from the sequence data that this is a third human TASK (TWIK-related acid sensitive K(+) channel) gene.\n","Most Ca2+-permeable ion channels are inhibited by increases in the intracellular Ca2+ concentration ([Ca2+]i), thus preventing potentially deleterious rises in [Ca2+]i.\n","WNK (with no lysine [K]) protein kinases were named for their unique active site organization.\n","Immunohistochemical staining using rabbit antisera against a synthetic peptide of UP-II and against total UPs showed UP reactivity in 39.5% (17 of 43 cases) of conventional TCCs, 12.8% (5 of 39) of bilharzial-related TCCs, and 2.7% (1 of 36) of bilharzial-related squamous cell carcinomas (SCCs).\n","Notch is a transmembrane protein involved in cell-fate decisions, and the cytoplasmic domain of Notch (NotchIC) targets CBF1.\n","Analysis of the predicted amino acid sequence revealed 13 nonidentical repeats similar to repeats found in the A subunit of PP2A (PP2AA).\n","Molecular cloning of mouse Lrp7(Lr3) cDNA and chromosomal mapping of orthologous genes in mouse and human.\n","The human regulator of G-protein signaling protein 6 gene (RGS6) maps between markers WI-5202 and D14S277 on chromosome 14q24.3.\n","Identification of a zeta-crystallin (quinone reductase)-like 1 gene (CRYZL1) mapped to human chromosome 21q22.1.\n","However, upon coexpression of arrestin-2 (beta-arrestin-1) or arrestin-3 (beta-arrestin-2), internalization of the alpha2b AR was dramatically enhanced and redistribution of receptors to clathrin coated vesicles and endosomes was observed.\n","Adrenoleukodystrophy-related protein can compensate functionally for adrenoleukodystrophy protein deficiency (X-ALD): implications for therapy.\n","Linkage analysis gave a significantly positive two-point LOD score (Z) at marker D13S175 (maximum Z [Zmax]=>7.0; maximum recombination frequency [thetamax] =0).\n","In family E, insertion of a C at nucleotide 1137 (1137insC) introduced a novel BstXI site, causing a frameshift at codon 380.\n","It is characterized by defective, very long chain fatty acid (VLCFA) beta-oxidation, resulting in progressive cerebral demyelination.\n","Using confocal microscopy of live PC12 cells, transiently transfected with a chimera of green fluorescent protein (GFP) fused to the N-terminus of centaurin-alpha1 (GFP-centaurin-alpha1), we demonstrated the rapid plasma membrane recruitment of cytosolic GFP-centaurin-alpha1 following stimulation with either nerve growth factor or epidermal growth factor.\n","This recruitment was dependent on the centaurin-alpha1 pleckstrin homology domains and was blocked by the PtdIns(4,5)P2 3-kinase (PI 3-kinase) inhibitors wortmannin (100 nM) and LY294002 (50 microM), and also by co-expression with a dominant negative p85.\n","The mutation in patients with Huntington's disease is an expanded CAG/polyglutamine repeat in huntingtin, a protein of unknown function with a relative molecular mass of 350,000 (M(r) 350K).\n","Here we demonstrate a critical role for the von Hippel-Lindau (VHL) tumour suppressor gene product pVHL in HIF-1 regulation.\n","We found that the androgen-independent prostate cancer cell line PC-3 undergoes terminal differentiation and apoptosis after treatment with sodium butyrate (NaBu).\n","While characterizing Eps15 partners, we identified a 48-kDa polypeptide (p48) which was precipitated by Eps15-derived glutathione S-transferase fusion proteins.\n","We previously described a novel TGF-beta family member, myostatin (encoded by the gene Mstn, formerly Gdf8), that has an essential role in regulating skeletal muscle mass.\n","The Drosophila latheo (lat) gene was identified in a behavioral screen for olfactory memory mutants.\n","Molecular cloning and characterization of a cDNA encoding the human leucocyte vacuolar protein sorting (h1Vps45).\n","We previously demonstrated that the intracellular third loop (i3 loop) of angiotensin II type 2 receptor (AT2) plays a key role in mediating the biological functions of this receptor.\n","Sgt1p also associates with SCF (Skp1p/Cdc53p/F box protein) ubiquitin ligase. sgt1-5 (G1 allele) mutants are defective in Sic1p turnover in vivo and Cln1p ubiquitination in vitro.\n","This phenotype also was caused by a recessive mutation, designated strigosus (stri).\n","In this study, by the generation of a specific monoclonal antibody, we identified p75/AIRM1 (for adhesion inhibitory receptor molecule 1), a novel inhibitory receptor that is mostly confined to human natural killer cells. p75/AIRM1 is a 75-kD glycoprotein that, upon sodium pervanadate treatment, becomes tyrosine phosphorylated and associates to src homology 2 domain-bearing protein tyrosine phosphatase (SHP)-1.\n","In vitro the lambda light chain dimer efficiently activated the alternative pathway of complement (AP).\n","We have identified and characterized a novel member of the human interleukin-1 gene family (IL1HY1).\n","Human phosphoribosylformylglycineamide amidotransferase (FGARAT): regional mapping, complete coding sequence, isolation of a functional genomic clone, and DNA sequence analysis.\n","One notable exception is the phosphoribosylformylglycineamide amidotransferase (FGARAT) gene, which encodes the fourth step of this pathway.\n","A novel RNA-binding nuclear protein that interacts with the fragile X mental retardation (FMR1) protein.\n","Interaction of c-Jun amino-terminal kinase interacting protein-1 with p190 rhoGEF and its localization in differentiated neurons. c-Jun amino-terminal kinase (JNK) interacting protein-1 (JIP-1) was originally identified as a cytoplasmic inhibitor of JNK.\n","We have identified a novel human kinase SNAK (SNARE kinase) that specifically phosphorylates the nonneuronal t-SNARE SNAP-23 in vivo.\n","This region shows frequent loss of heterozygosity (LOH) in carcinomas arising in several developmentally related tissues, including the esophagus, head and neck, lung, and urinary bladder.\n","A., and Jamrich, M. (1997) Nature 387, 603-607), binds to the photoreceptor conserved element-1 (PCE-1/Ret 1) in the photoreceptor cell-specific arrestin promoter and stimulates gene expression.\n","CDPX2 is due to mutations affecting a delta8-delta7 sterol isomerase (EBP, emopamil binding protein, at Xp11.22-p11.23) that functions downstream of NSDHL in a later step of cholesterol biosynthesis.\n","This locus encoded for the Neuroendocrine Secretory Protein with an apparent molecular weight of 55,000 (NESP55), which is transcribed exclusively from the maternal allele.\n","We have identified a novel member of the BASH/SLP-76 immunoreceptor-coupled adaptor family expressed in mast cells, termed MIST (for mast cell immunoreceptor signal transducer), which has later been found to be identical to a recently reported cytokine-dependent hemopoietic cell linker, Clnk.\n","In this study, we have identified a novel human gene based on homology to the Escherichia coli sialic acid synthase gene (neuB).\n","The encoded putative PPP4 regulatory subunit (termed PPP4R2), comprising 453 amino acids, had a molecular mass of 50.4 kDa.\n","We previously reported that expression of polyglutamine-expanded huntingtin induces apoptosis via c-Jun amino-terminal kinase (JNK) activation in HN33 cells (Liu, Y. F. (1998) J. Biol. Chem. 273, 28873-28822).\n","Mitogen-activated protein kinase upstream kinase/dual leucine zipper-bearing kinase/leucine-zipper protein kinase (MUK/DLK/ZPK) is a MAPKKK class protein kinase that induces JNK/SAPK activation.\n","Distinct roles of the NH2- and COOH-terminal domains of the protein inhibitor of activated signal transducer and activator of transcription (STAT) 1 (PIAS1) in cytokine-induced PIAS1-Stat1 interaction.\n"]}]},{"cell_type":"markdown","source":["## Test on AB3P corpus"],"metadata":{"id":"UtGcTLhPLwjw"}},{"cell_type":"code","source":["# change annotations offset:\n","for document in ab3p.documents:\n","  for passage in document.passages:\n","    passage_offset = passage.offset\n","    for annotation in passage.annotations:\n","      annotation.locations[0].offset -= passage_offset\n","print('substracted passage offset')"],"metadata":{"id":"AGURIUN8JUJP","executionInfo":{"status":"ok","timestamp":1650944931080,"user_tz":-420,"elapsed":425,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae8d98b3-c22e-4aa4-c96d-8c967cdccea2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["substracted passage offset\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iQFVxc323pp","executionInfo":{"status":"ok","timestamp":1650944963863,"user_tz":-420,"elapsed":32464,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"65c3bdaa-af1a-4b4e-b883-d56a86d9419c","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Process passage 0\n","Process passage 50\n","Process passage 100\n","Process passage 150\n","Process passage 200\n","Process passage 250\n","Process passage 300\n","Process passage 350\n","Process passage 400\n","Process passage 450\n","Process passage 500\n","Process passage 550\n","Process passage 600\n","Process passage 650\n","Process passage 700\n","Process passage 750\n","Process passage 800\n","Process passage 850\n","Process passage 900\n","Process passage 950\n","Process passage 1000\n","Process passage 1050\n","Process passage 1100\n","Process passage 1150\n","Process passage 1200\n"]}],"source":["#@title create data frame\n","sentence_test_corpus = []\n","tag_test_corpus = []\n","tag_prediction = []\n","id_test_corpus = []\n","\n","i = 0\n","\n","\n","for document in ab3p.documents:\n","\n","  passage = document.passages[1]\n","  new_passage = ExtractSentenceWithAcronym(passage)\n","  predictionList = PredictSentenceWithABBR(passage.text)\n","  for sentence in new_passage.sentences:\n","    # get sentence\n","    sentence_test_corpus.append(sentence.text)\n","    # get id\n","    id_test_corpus.append(i)\n","    # get truth\n","\n","    if sentence.annotations:\n","      tag_test_corpus.append(True)\n","\n","    else:\n","      tag_test_corpus.append(False)\n","\n","    #get prediction\n","    tag_prediction.append(False)\n","    for predicted_sentence in predictionList:\n","      if sentence.text in predicted_sentence or predicted_sentence in sentence.text:\n","        tag_prediction[-1] = True\n","  if i%50 == 0:\n","    print(f'Process passage {i}')          \n","      \n","  i+=1\n","contigency_df = pd.DataFrame({'sentence':sentence_test_corpus,'truth':tag_test_corpus,'prediction':tag_prediction, 'id':id_test_corpus})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"59mUVpoe7T5l","executionInfo":{"status":"ok","timestamp":1650944963864,"user_tz":-420,"elapsed":21,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"8ae10635-d891-4ec2-c4d5-de35dd28a697"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               sentence  truth  prediction  \\\n","0     OBJECTIVE: To generate a classification of met...  False       False   \n","1     METHODS: Multiple search strategies were emplo...  False       False   \n","2     RESULTS: All methods available were classified...  False       False   \n","3     The first method group, impute or adjust for m...  False       False   \n","4     The second group, correct imperfect reference ...  False       False   \n","...                                                 ...    ...         ...   \n","9212  Mutation of S383 and S387 abolished the phosph...  False       False   \n","9213  These results were confirmed by use of phospho...  False       False   \n","9214  Furthermore, the ability to correct FA-G mutan...  False       False   \n","9215  S387A mutant abolished FANCG fusion protein ph...  False       False   \n","9216  The FA pathway, of which FANCG is a part, is h...  False       False   \n","\n","        id  \n","0        0  \n","1        0  \n","2        0  \n","3        0  \n","4        0  \n","...    ...  \n","9212  1249  \n","9213  1249  \n","9214  1249  \n","9215  1249  \n","9216  1249  \n","\n","[9217 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-968d6726-8626-4c97-8c2b-9f976a72ea16\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>truth</th>\n","      <th>prediction</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>OBJECTIVE: To generate a classification of met...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>METHODS: Multiple search strategies were emplo...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>RESULTS: All methods available were classified...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The first method group, impute or adjust for m...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The second group, correct imperfect reference ...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9212</th>\n","      <td>Mutation of S383 and S387 abolished the phosph...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1249</td>\n","    </tr>\n","    <tr>\n","      <th>9213</th>\n","      <td>These results were confirmed by use of phospho...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1249</td>\n","    </tr>\n","    <tr>\n","      <th>9214</th>\n","      <td>Furthermore, the ability to correct FA-G mutan...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1249</td>\n","    </tr>\n","    <tr>\n","      <th>9215</th>\n","      <td>S387A mutant abolished FANCG fusion protein ph...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1249</td>\n","    </tr>\n","    <tr>\n","      <th>9216</th>\n","      <td>The FA pathway, of which FANCG is a part, is h...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1249</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9217 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-968d6726-8626-4c97-8c2b-9f976a72ea16')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-968d6726-8626-4c97-8c2b-9f976a72ea16 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-968d6726-8626-4c97-8c2b-9f976a72ea16');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}],"source":["contigency_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdzH_OLb3xpe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944964579,"user_tz":-420,"elapsed":733,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"5d0b2d6c-3601-4682-fdfe-2496f028b46a"},"outputs":[{"output_type":"stream","name":"stdout","text":["True Positive:762\n"," False Positve:32\n"," True Negative:8321\n"," False Negative:102\n","\n"]}],"source":["#@ calculate TP, TN, FN, FP\n","TP = 0\n","TN = 0\n","FN = 0\n","FP = 0\n","n = contigency_df.shape[0]\n","for i in range(n):\n","  if contigency_df.truth[i] == True and contigency_df.prediction[i] == True:\n","    TP += 1\n","  elif contigency_df.truth[i] == False and contigency_df.prediction[i] == False:\n","    TN += 1\n","  elif contigency_df.truth[i] == True and contigency_df.prediction[i] == False:\n","    FN += 1\n","  elif contigency_df.truth[i] == False and contigency_df.prediction[i] == True:\n","    FP += 1\n","print(f'True Positive:{TP}\\n False Positve:{FP}\\n True Negative:{TN}\\n False Negative:{FN}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJ6KmamA6R8J","executionInfo":{"status":"ok","timestamp":1650944964580,"user_tz":-420,"elapsed":11,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"814e8ef7-3f26-4c4b-ccb5-c829697697d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["overall accuracy: 0.9854616469567105 \n"," precision: 0.9596977329974811 \n"," recall: 0.8819444444444444\n"]}],"source":["#@ metric\n","accuracy = (TP+TN)/(TP+TN+FN+FP)\n","precision = TP/(TP+FP)\n","recall = TP/(TP+FN)\n","print(f'overall accuracy: {accuracy} \\n precision: {precision} \\n recall: {recall}')"]},{"cell_type":"markdown","source":["## Predict new sentence"],"metadata":{"id":"1I2sBB9u52-S"}},{"cell_type":"code","source":["input = input()"],"metadata":{"id":"oHc2LAFID7QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PredictSentenceWithABBR(input)"],"metadata":{"id":"ZxIza2W66grZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(input.split())):\n","  print(input.split()[i],PredictToken(input)[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8kt8ezzD-mg","executionInfo":{"status":"ok","timestamp":1650912161670,"user_tz":-420,"elapsed":5369,"user":{"displayName":"Hoàng Hiệp Đặng","userId":"09498793559338135353"}},"outputId":"6911e994-01d8-4ce8-b728-b46e5bc75796"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parathyroid S\n","hormone-regulated O\n","production O\n","of O\n","stem O\n","cell O\n","factor O\n","in O\n","human O\n","osteoblasts O\n","and O\n","osteoblast-like O\n","cells. O\n","We B\n","investigated I\n","stem I\n","cell I\n","factor I\n","(SCF) I\n","expression I\n","in I\n","osteoblasts I\n","because I\n","mast I\n","cells, I\n","which I\n","occur I\n","ectopically I\n","in I\n","hyperparathyroid I\n","bone, I\n","are I\n","induced I\n","by I\n","SCF. I\n","Nontransformed S\n","osteoblasts O\n","and O\n","Saos2 O\n","or O\n","MG63 O\n","cells O\n","expressed O\n","SCF O\n","in O\n","response O\n","to O\n","PTH. O\n","Western S\n","analysis O\n","showed O\n","only O\n","large, O\n","cell-associated O\n","isoforms, O\n","Mrs O\n","approximately O\n","40-48 O\n","kD. O\n","Transfection S\n","of O\n","MG63 O\n","cells O\n","with O\n","plasmids O\n","expressing O\n","antisense O\n","SCF O\n","mRNA O\n","eliminated O\n","immunoreactive O\n","SCF. O\n","Sequencing S\n","osteoblast O\n","SCF O\n","cDNAs O\n","showed O\n","that O\n","exon O\n","6 O\n","was O\n","omitted. O\n","mRNAs S\n","without O\n","exon O\n","6 O\n","produce O\n","membrane-associated O\n","SCF O\n","isoforms O\n","in O\n","rodents, O\n","suggesting O\n","that O\n","human O\n","SCFs O\n","are O\n","processed O\n","similarly. O\n","The S\n","major O\n","osteoblastic O\n","SCF O\n","mRNA, O\n","approximately O\n","5 O\n","kB, O\n","was O\n","augmented O\n","by O\n","PTH. O\n","Neither S\n","protein O\n","or O\n","mRNA O\n","was O\n","increased O\n","by O\n","vitamin O\n","D, O\n","however, O\n","6-7 O\n","kB O\n","transcripts O\n","were O\n","predominant O\n","in O\n","other O\n","tissues O\n","but O\n","not O\n","detectable O\n","in O\n","osteoblasts. O\n","We S\n","conclude O\n","that O\n","osteoblasts O\n","express O\n","SCF O\n","in O\n","response O\n","to O\n","PTH, O\n","with O\n","mRNA O\n","and O\n","protein O\n","processing O\n","differences O\n","relative O\n","to O\n","other O\n","cells. O\n","SCF S\n","stimulates O\n","osteoclasts, O\n","suggesting O\n","that O\n","PTH-induced O\n","osteoblastic O\n","SCF O\n","functions O\n","to O\n","accelerate O\n","bone O\n","turnover. O\n","Mast S\n","cells O\n","may O\n","occur O\n","due O\n","to O\n","SCF O\n","overexpression O\n","at O\n","extreme O\n","PTH O\n","levels. O\n"]}]}]}